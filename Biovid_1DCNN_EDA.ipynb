{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation \n",
    "===============================\n",
    "\n",
    "**Author**: [Clara Martinez](https://github.com/moonblume/LIVIA.git)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique that enables knowledge transfer\n",
    "from large, computationally expensive models to smaller ones without\n",
    "losing validity. This allows for deployment on less powerful hardware,\n",
    "making evaluation faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librairies\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from typing import List, Union, Tuple, Any\n",
    "import statistics\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset\n",
    "================\n",
    "\n",
    "Fisrt, I focus on the physiological signals of the Biovid dataset. In one sample, we have access to 6 classes associated with 0 to 4 pain levels :  \n",
    "\n",
    "Time: This could be the timestamp or time index when the signal was recorded.\n",
    "\n",
    "GSR (Galvanic Skin Response): A measure of the electrical conductance of the skin, which varies with the moisture level of the skin. It's often associated with emotional arousal.\n",
    "\n",
    "ECG (Electrocardiogram): A recording of the electrical activity of the heart over time. It typically consists of waves representing the depolarization and repolarization of the heart muscle during each heartbeat.\n",
    "\n",
    "EMG (Electromyography) - Trapezius: Measures the electrical activity produced by skeletal muscles. The trapezius muscle is a large superficial muscle that extends longitudinally from the occipital bone to the lower thoracic vertebrae and laterally to the spine of the scapula.\n",
    "\n",
    "EMG - Corrugator: Electromyography signal from the corrugator supercilii muscle, which is a small facial muscle involved in frowning and expressing negative emotions.\n",
    "\n",
    "EMG - Zygomaticus: Electromyography signal from the zygomaticus major muscle, which is involved in smiling and expressing positive emotions.  \n",
    "    \n",
    "    \n",
    "Our objective is to predict the pain level of input signals. One signal corresponds to one csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>072414_m_23-PA2-034_bio.csv</td>\n",
       "      <td>[6.966839, 6.966161, 6.966, 6.966839, 6.966161...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081609_w_40-PA2-028_bio.csv</td>\n",
       "      <td>[0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081714_m_36-PA2-065_bio.csv</td>\n",
       "      <td>[6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102514_w_40-PA2-046_bio.csv</td>\n",
       "      <td>[1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120514_w_56-PA2-019_bio.csv</td>\n",
       "      <td>[2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CSV name  \\\n",
       "0  072414_m_23-PA2-034_bio.csv   \n",
       "1  081609_w_40-PA2-028_bio.csv   \n",
       "2  081714_m_36-PA2-065_bio.csv   \n",
       "3  102514_w_40-PA2-046_bio.csv   \n",
       "4  120514_w_56-PA2-019_bio.csv   \n",
       "\n",
       "                                         GSR signals  Pain level  \n",
       "0  [6.966839, 6.966161, 6.966, 6.966839, 6.966161...           2  \n",
       "1  [0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...           2  \n",
       "2  [6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...           2  \n",
       "3  [1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...           2  \n",
       "4  [2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...           2  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory containing the CSV files\n",
    "biosignals_path = '/home/ens/AU59350/LIVIA/physio/physio_organised/'\n",
    "\n",
    "# Initialize an empty list to store data for DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over each pain level directory\n",
    "for pain_level in os.listdir(biosignals_path):\n",
    "    pain_level_dir = os.path.join(biosignals_path, pain_level)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(pain_level_dir):\n",
    "        # Iterate over each CSV file in the pain level directory\n",
    "        for csv_file in os.listdir(pain_level_dir):\n",
    "            # Check if it's a CSV file\n",
    "            if csv_file.endswith('.csv'):\n",
    "                csv_path = os.path.join(pain_level_dir, csv_file)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(csv_path, sep='\\t')\n",
    "                # Extract GSR values\n",
    "                gsr_signal = df['gsr'].values\n",
    "                # Append the CSV name, GSR signals, and Pain level to the data list\n",
    "                data.append({'CSV name': csv_file, 'GSR signals': gsr_signal, 'Pain level': int(pain_level)})\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         CSV name  \\\n",
      "0     072414_m_23-PA2-034_bio.csv   \n",
      "1     081609_w_40-PA2-028_bio.csv   \n",
      "2     081714_m_36-PA2-065_bio.csv   \n",
      "3     102514_w_40-PA2-046_bio.csv   \n",
      "4     120514_w_56-PA2-019_bio.csv   \n",
      "...                           ...   \n",
      "8695  082909_m_47-BL1-085_bio.csv   \n",
      "8696  081609_w_40-BL1-090_bio.csv   \n",
      "8697  091809_w_43-BL1-097_bio.csv   \n",
      "8698  112016_m_25-BL1-091_bio.csv   \n",
      "8699  083013_w_47-BL1-086_bio.csv   \n",
      "\n",
      "                                            GSR signals  Pain level  \n",
      "0     [6.966839, 6.966161, 6.966, 6.966839, 6.966161...           2  \n",
      "1     [0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...           2  \n",
      "2     [6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...           2  \n",
      "3     [1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...           2  \n",
      "4     [2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...           2  \n",
      "...                                                 ...         ...  \n",
      "8695  [3.184, 3.184399, 3.1846, 3.184, 3.184, 3.184,...           0  \n",
      "8696  [0.885, 0.885, 0.885, 0.885, 0.885, 0.885, 0.8...           0  \n",
      "8697  [2.208, 2.208, 2.208, 2.208, 2.208, 2.208, 2.2...           0  \n",
      "8698  [3.252322, 3.252678, 3.252, 3.252323, 3.253, 3...           0  \n",
      "8699  [1.571, 1.571, 1.571, 1.571, 1.571, 1.571, 1.5...           0  \n",
      "\n",
      "[8700 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of number of pain level included in the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         CSV name  \\\n",
      "5220  080314_w_25-PA4-067_bio.csv   \n",
      "5221  092009_m_54-PA4-042_bio.csv   \n",
      "5222  071709_w_23-PA4-071_bio.csv   \n",
      "5223  082809_m_26-PA4-005_bio.csv   \n",
      "5224  112909_w_20-PA4-080_bio.csv   \n",
      "...                           ...   \n",
      "8695  082909_m_47-BL1-085_bio.csv   \n",
      "8696  081609_w_40-BL1-090_bio.csv   \n",
      "8697  091809_w_43-BL1-097_bio.csv   \n",
      "8698  112016_m_25-BL1-091_bio.csv   \n",
      "8699  083013_w_47-BL1-086_bio.csv   \n",
      "\n",
      "                                            GSR signals  Pain level  \n",
      "5220  [4.521143, 4.522, 4.522, 4.522, 4.522, 4.522, ...           4  \n",
      "5221  [1.673, 1.673, 1.673, 1.673, 1.673, 1.673, 1.6...           4  \n",
      "5222  [4.326707, 4.327, 4.326292, 4.326708, 4.327, 4...           4  \n",
      "5223  [3.913801, 3.913, 3.913, 3.913, 3.912801, 3.91...           4  \n",
      "5224  [2.633, 2.633728, 2.634, 2.633272, 2.633728, 2...           4  \n",
      "...                                                 ...         ...  \n",
      "8695  [3.184, 3.184399, 3.1846, 3.184, 3.184, 3.184,...           0  \n",
      "8696  [0.885, 0.885, 0.885, 0.885, 0.885, 0.885, 0.8...           0  \n",
      "8697  [2.208, 2.208, 2.208, 2.208, 2.208, 2.208, 2.2...           0  \n",
      "8698  [3.252322, 3.252678, 3.252, 3.252323, 3.253, 3...           0  \n",
      "8699  [1.571, 1.571, 1.571, 1.571, 1.571, 1.571, 1.5...           0  \n",
      "\n",
      "[3480 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to keep rows where pain level is not equal to 1, 2, or 3\n",
    "df = df[~df['Pain level'].isin([1, 2, 3])]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps for GSR DataFrame include tasks such as handling missing values, smoothing the signal to reduce noise in the GSR signal 9(Savitzky-Golay filtering), removing outliers (z-score), and normalizing the data between a specified range, such as [0, 1] or [-1, 1] helping comparison across different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>080314_w_25-PA4-067_bio.csv</td>\n",
       "      <td>[0.0, 0.000784955924601847, 0.0012131137016623...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>092009_m_54-PA4-042_bio.csv</td>\n",
       "      <td>[0.21654547886192313, 0.21654547886192313, 0.2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>071709_w_23-PA4-071_bio.csv</td>\n",
       "      <td>[1.0, 0.9983642739356123, 0.9979709555643436, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223</th>\n",
       "      <td>082809_m_26-PA4-005_bio.csv</td>\n",
       "      <td>[0.3629402970779174, 0.36178416384520784, 0.36...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>112909_w_20-PA4-080_bio.csv</td>\n",
       "      <td>[0.16994520796566095, 0.1741950993228739, 0.17...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         CSV name  \\\n",
       "5220  080314_w_25-PA4-067_bio.csv   \n",
       "5221  092009_m_54-PA4-042_bio.csv   \n",
       "5222  071709_w_23-PA4-071_bio.csv   \n",
       "5223  082809_m_26-PA4-005_bio.csv   \n",
       "5224  112909_w_20-PA4-080_bio.csv   \n",
       "\n",
       "                                            GSR signals  Pain level  \n",
       "5220  [0.0, 0.000784955924601847, 0.0012131137016623...           4  \n",
       "5221  [0.21654547886192313, 0.21654547886192313, 0.2...           4  \n",
       "5222  [1.0, 0.9983642739356123, 0.9979709555643436, ...           4  \n",
       "5223  [0.3629402970779174, 0.36178416384520784, 0.36...           4  \n",
       "5224  [0.16994520796566095, 0.1741950993228739, 0.17...           4  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to preprocess GSR signals\n",
    "def preprocess_gsr_signal(gsr_signal):\n",
    "    # Handle missing values (if any)\n",
    "    gsr_signal = np.array(gsr_signal)  # Convert to NumPy array\n",
    "    gsr_signal = gsr_signal[~np.isnan(gsr_signal)]  # Remove NaN values\n",
    "    \n",
    "    # Check if the length of the signal is sufficient for smoothing\n",
    "    if len(gsr_signal) < 5:\n",
    "        # If the signal is too short, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    try:\n",
    "        # Smoothing using Savitzky-Golay filter\n",
    "        gsr_signal_smooth = savgol_filter(gsr_signal, window_length=5, polyorder=2)\n",
    "    except ValueError:\n",
    "        # If an error occurs during smoothing, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    # Removing outliers based on Z-scores\n",
    "    z_scores = (gsr_signal_smooth - gsr_signal_smooth.mean()) / gsr_signal_smooth.std()\n",
    "    gsr_signal_smooth_no_outliers = gsr_signal_smooth[(z_scores < 3)]\n",
    "    \n",
    "    # Normalization\n",
    "    if len(gsr_signal_smooth_no_outliers) > 0:\n",
    "        gsr_signal_normalized = (gsr_signal_smooth_no_outliers - gsr_signal_smooth_no_outliers.min()) / \\\n",
    "                                 (gsr_signal_smooth_no_outliers.max() - gsr_signal_smooth_no_outliers.min())\n",
    "    else:\n",
    "        # If there are no valid values after removing outliers, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    return gsr_signal_normalized\n",
    "\n",
    "# Apply preprocessing to each row in the DataFrame\n",
    "df['GSR signals'] = df['GSR signals'].apply(preprocess_gsr_signal)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([2784, 1, 2816])\n",
      "X_test_tensor shape: torch.Size([696, 1, 2816])\n",
      "y_train_tensor shape: torch.Size([2784])\n",
      "y_test_tensor shape: torch.Size([696])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the data\n",
    "max_length = max(len(signal) for signal in df['GSR signals'])  # Find the maximum length of GSR signals\n",
    "\n",
    "# Pad or truncate the GSR signals to the maximum length\n",
    "gsr_signals = np.array([np.pad(signal, (0, max_length - len(signal))) if len(signal) < max_length else signal[:max_length] for signal in df['GSR signals']])\n",
    "\n",
    "pain_levels = df['Pain level'].values\n",
    "\n",
    "# Step 2: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gsr_signals, pain_levels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  \n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Check the shape of tensors\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1, 2816])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dimensionality of the tensor\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension (29) represents the batch size, indicating that there are 29 samples in the batch.  \n",
    "The second dimension (1) represents the number of channels. In this case, there is only one channel.  \n",
    "The third dimension (1) represents the length of the input data for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data has a shape of (batch_size, channels, sequence_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D- CNN model\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I build and train a Convolutional Neural Network (CNN) using PyTorch for a classification task of pain levels. It defines the model architecture, sets up training parameters, prepares data tensors, performs K-Fold cross-validation, trains the model, evaluates it on validation sets, and finally evaluates its performance on a test set. \n",
    "\n",
    "It follows thoses steps :\n",
    "\n",
    "1) **Define the CNN Model (Conv1D_model):**\n",
    "\n",
    "This model consists of two convolutional layers followed by max-pooling layers and two fully connected layers. The first convolutional layer (conv1) has 32 output channels, a kernel size of 5, and a stride of 2. After each convolutional layer, a Rectified Linear Unit (ReLU) activation function is applied (relu1, relu2). Max-pooling layers (maxpool1, maxpool2) with kernel size 2 are used to reduce the spatial dimensions. The output of the convolutional layers is flattened and passed through two fully connected layers (fc1, fc2) for classification. The output layer does not have an activation function (sigmoid) because nn.CrossEntropyLoss already includes a softmax layer.\n",
    "\n",
    "2) **Define Learning Parameters:**\n",
    "\n",
    "Learning rate (lr), number of epochs (num_epochs), batch size (batch_size), and number of classes (num_classes) are defined.\n",
    "\n",
    "3) **Define Loss Function and Optimizer:**\n",
    "\n",
    "Cross-entropy loss (nn.CrossEntropyLoss()) is used as the loss function.\n",
    "Stochastic Gradient Descent (SGD) optimizer (optim.SGD) with momentum is used for optimization.\n",
    "\n",
    "4) **Prepare Training and Testing Data:**\n",
    "\n",
    "Synthetic data tensors (X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor) are created. Replace them with your actual data.\n",
    "\n",
    "5) **Perform K-Fold Cross-Validation:**\n",
    "\n",
    "K-Fold cross-validation with 5 folds (k_folds) is performed using KFold from sklearn.model_selection.\n",
    "Data is split into training and validation sets for each fold.\n",
    "Training and validation data are loaded into PyTorch DataLoader objects.\n",
    "\n",
    "6) **Train the Model:**\n",
    "\n",
    "The model is trained for the specified number of epochs.\n",
    "Training is done on the training data using mini-batches obtained from the train_loader.\n",
    "After each epoch, the model is evaluated on the validation set to monitor performance.\n",
    "\n",
    "7) **Evaluate the Model on Test Data:**\n",
    "\n",
    "After cross-validation, the final trained model is evaluated on the test set to assess its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 4 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     87\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 88\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 4 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Define the Conv1D model\n",
    "class Conv1D_model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Conv1D_model, self).__init__()\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(22336, 512)  \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Define learning parameters\n",
    "lr = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 1024\n",
    "num_classes = 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define training and testing tensors \n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  \n",
    "y_train_tensor = torch.LongTensor(y_train)  \n",
    "y_test_tensor = torch.LongTensor(y_test)    \n",
    "\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_tensor)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = X_train_tensor[train_index], X_train_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_tensor[train_index], y_train_tensor[val_index]\n",
    "\n",
    "    # Create DataLoader for training and validation sets\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Conv1D_model(num_classes=num_classes)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4822\n",
      "MAE: 0.5178\n",
      "RMSE: 0.7196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ens/AU59350/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    y_test_tensor_sampled = y_test_tensor[:len(predictions)]\n",
    "    accuracy = torch.mean((predictions == y_test_tensor_sampled).float()).item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Calculate MAE and RMSE\n",
    "    mae = mean_absolute_error(y_test_tensor_sampled, predictions)\n",
    "    rmse = mean_squared_error(y_test_tensor_sampled, predictions, squared=False)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
