{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation \n",
    "===============================\n",
    "\n",
    "**Author**: [Clara Martinez](https://github.com/moonblume/LIVIA.git)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique that enables knowledge transfer\n",
    "from large, computationally expensive models to smaller ones without\n",
    "losing validity. This allows for deployment on less powerful hardware,\n",
    "making evaluation faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librairies\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from typing import List, Union, Tuple, Any\n",
    "import statistics\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset\n",
    "================\n",
    "\n",
    "Fisrt, I focus on the physiological signals of the Biovid dataset. In one sample, we have access to 6 classes associated with 0 to 4 pain levels :  \n",
    "\n",
    "Time: This could be the timestamp or time index when the signal was recorded.\n",
    "\n",
    "GSR (Galvanic Skin Response): A measure of the electrical conductance of the skin, which varies with the moisture level of the skin. It's often associated with emotional arousal.\n",
    "\n",
    "ECG (Electrocardiogram): A recording of the electrical activity of the heart over time. It typically consists of waves representing the depolarization and repolarization of the heart muscle during each heartbeat.\n",
    "\n",
    "EMG (Electromyography) - Trapezius: Measures the electrical activity produced by skeletal muscles. The trapezius muscle is a large superficial muscle that extends longitudinally from the occipital bone to the lower thoracic vertebrae and laterally to the spine of the scapula.\n",
    "\n",
    "EMG - Corrugator: Electromyography signal from the corrugator supercilii muscle, which is a small facial muscle involved in frowning and expressing negative emotions.\n",
    "\n",
    "EMG - Zygomaticus: Electromyography signal from the zygomaticus major muscle, which is involved in smiling and expressing positive emotions.  \n",
    "    \n",
    "    \n",
    "Our objective is to predict the pain level of input signals. One signal corresponds to one csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>072414_m_23-PA2-034_bio.csv</td>\n",
       "      <td>[6.966839, 6.966161, 6.966, 6.966839, 6.966161...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081609_w_40-PA2-028_bio.csv</td>\n",
       "      <td>[0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081714_m_36-PA2-065_bio.csv</td>\n",
       "      <td>[6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102514_w_40-PA2-046_bio.csv</td>\n",
       "      <td>[1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120514_w_56-PA2-019_bio.csv</td>\n",
       "      <td>[2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CSV name  \\\n",
       "0  072414_m_23-PA2-034_bio.csv   \n",
       "1  081609_w_40-PA2-028_bio.csv   \n",
       "2  081714_m_36-PA2-065_bio.csv   \n",
       "3  102514_w_40-PA2-046_bio.csv   \n",
       "4  120514_w_56-PA2-019_bio.csv   \n",
       "\n",
       "                                         GSR signals  Pain level  \n",
       "0  [6.966839, 6.966161, 6.966, 6.966839, 6.966161...           2  \n",
       "1  [0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...           2  \n",
       "2  [6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...           2  \n",
       "3  [1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...           2  \n",
       "4  [2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...           2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory containing the CSV files\n",
    "biosignals_path = '/home/ens/AU59350/LIVIA/physio/physio_organised/'\n",
    "\n",
    "# Initialize an empty list to store data for DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over each pain level directory\n",
    "for pain_level in os.listdir(biosignals_path):\n",
    "    pain_level_dir = os.path.join(biosignals_path, pain_level)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(pain_level_dir):\n",
    "        # Iterate over each CSV file in the pain level directory\n",
    "        for csv_file in os.listdir(pain_level_dir):\n",
    "            # Check if it's a CSV file\n",
    "            if csv_file.endswith('.csv'):\n",
    "                csv_path = os.path.join(pain_level_dir, csv_file)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(csv_path, sep='\\t')\n",
    "                # Extract GSR values\n",
    "                gsr_signal = df['gsr'].values\n",
    "                # Append the CSV name, GSR signals, and Pain level to the data list\n",
    "                data.append({'CSV name': csv_file, 'GSR signals': gsr_signal, 'Pain level': int(pain_level)})\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of number of pain level included in the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         CSV name  \\\n",
      "5220  080314_w_25-PA4-067_bio.csv   \n",
      "5221  092009_m_54-PA4-042_bio.csv   \n",
      "5222  071709_w_23-PA4-071_bio.csv   \n",
      "5223  082809_m_26-PA4-005_bio.csv   \n",
      "5224  112909_w_20-PA4-080_bio.csv   \n",
      "...                           ...   \n",
      "8695  082909_m_47-BL1-085_bio.csv   \n",
      "8696  081609_w_40-BL1-090_bio.csv   \n",
      "8697  091809_w_43-BL1-097_bio.csv   \n",
      "8698  112016_m_25-BL1-091_bio.csv   \n",
      "8699  083013_w_47-BL1-086_bio.csv   \n",
      "\n",
      "                                            GSR signals  Pain level  \n",
      "5220  [4.521143, 4.522, 4.522, 4.522, 4.522, 4.522, ...           4  \n",
      "5221  [1.673, 1.673, 1.673, 1.673, 1.673, 1.673, 1.6...           4  \n",
      "5222  [4.326707, 4.327, 4.326292, 4.326708, 4.327, 4...           4  \n",
      "5223  [3.913801, 3.913, 3.913, 3.913, 3.912801, 3.91...           4  \n",
      "5224  [2.633, 2.633728, 2.634, 2.633272, 2.633728, 2...           4  \n",
      "...                                                 ...         ...  \n",
      "8695  [3.184, 3.184399, 3.1846, 3.184, 3.184, 3.184,...           0  \n",
      "8696  [0.885, 0.885, 0.885, 0.885, 0.885, 0.885, 0.8...           0  \n",
      "8697  [2.208, 2.208, 2.208, 2.208, 2.208, 2.208, 2.2...           0  \n",
      "8698  [3.252322, 3.252678, 3.252, 3.252323, 3.253, 3...           0  \n",
      "8699  [1.571, 1.571, 1.571, 1.571, 1.571, 1.571, 1.5...           0  \n",
      "\n",
      "[3480 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to keep rows where pain level is not equal to 1, 2, or 3\n",
    "df = df[~df['Pain level'].isin([1, 2, 3])]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps for GSR DataFrame include tasks such as handling missing values, smoothing the signal to reduce noise in the GSR signal 9(Savitzky-Golay filtering), removing outliers (z-score), and normalizing the data between a specified range, such as [0, 1] or [-1, 1] helping comparison across different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>080314_w_25-PA4-067_bio.csv</td>\n",
       "      <td>[0.0, 0.000784955924601847, 0.0012131137016623...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>092009_m_54-PA4-042_bio.csv</td>\n",
       "      <td>[0.21654547886192313, 0.21654547886192313, 0.2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>071709_w_23-PA4-071_bio.csv</td>\n",
       "      <td>[1.0, 0.9983642739356123, 0.9979709555643436, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223</th>\n",
       "      <td>082809_m_26-PA4-005_bio.csv</td>\n",
       "      <td>[0.3629402970779174, 0.36178416384520784, 0.36...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>112909_w_20-PA4-080_bio.csv</td>\n",
       "      <td>[0.16994520796566095, 0.1741950993228739, 0.17...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         CSV name  \\\n",
       "5220  080314_w_25-PA4-067_bio.csv   \n",
       "5221  092009_m_54-PA4-042_bio.csv   \n",
       "5222  071709_w_23-PA4-071_bio.csv   \n",
       "5223  082809_m_26-PA4-005_bio.csv   \n",
       "5224  112909_w_20-PA4-080_bio.csv   \n",
       "\n",
       "                                            GSR signals  Pain level  \n",
       "5220  [0.0, 0.000784955924601847, 0.0012131137016623...           4  \n",
       "5221  [0.21654547886192313, 0.21654547886192313, 0.2...           4  \n",
       "5222  [1.0, 0.9983642739356123, 0.9979709555643436, ...           4  \n",
       "5223  [0.3629402970779174, 0.36178416384520784, 0.36...           4  \n",
       "5224  [0.16994520796566095, 0.1741950993228739, 0.17...           4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to preprocess GSR signals\n",
    "def preprocess_gsr_signal(gsr_signal):\n",
    "    # Handle missing values (if any)\n",
    "    gsr_signal = np.array(gsr_signal)  # Convert to NumPy array\n",
    "    gsr_signal = gsr_signal[~np.isnan(gsr_signal)]  # Remove NaN values\n",
    "    \n",
    "    # Check if the length of the signal is sufficient for smoothing\n",
    "    if len(gsr_signal) < 5:\n",
    "        # If the signal is too short, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    try:\n",
    "        # Smoothing using Savitzky-Golay filter\n",
    "        gsr_signal_smooth = savgol_filter(gsr_signal, window_length=5, polyorder=2)\n",
    "    except ValueError:\n",
    "        # If an error occurs during smoothing, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    # Removing outliers based on Z-scores\n",
    "    z_scores = (gsr_signal_smooth - gsr_signal_smooth.mean()) / gsr_signal_smooth.std()\n",
    "    gsr_signal_smooth_no_outliers = gsr_signal_smooth[(z_scores < 3)]\n",
    "    \n",
    "    # Normalization\n",
    "    if len(gsr_signal_smooth_no_outliers) > 0:\n",
    "        gsr_signal_normalized = (gsr_signal_smooth_no_outliers - gsr_signal_smooth_no_outliers.min()) / \\\n",
    "                                 (gsr_signal_smooth_no_outliers.max() - gsr_signal_smooth_no_outliers.min())\n",
    "    else:\n",
    "        # If there are no valid values after removing outliers, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    return gsr_signal_normalized\n",
    "\n",
    "# Apply preprocessing to each row in the DataFrame\n",
    "df['GSR signals'] = df['GSR signals'].apply(preprocess_gsr_signal)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         CSV name  \\\n",
      "5220  080314_w_25-PA4-067_bio.csv   \n",
      "5221  092009_m_54-PA4-042_bio.csv   \n",
      "5222  071709_w_23-PA4-071_bio.csv   \n",
      "5223  082809_m_26-PA4-005_bio.csv   \n",
      "5224  112909_w_20-PA4-080_bio.csv   \n",
      "...                           ...   \n",
      "8695  082909_m_47-BL1-085_bio.csv   \n",
      "8696  081609_w_40-BL1-090_bio.csv   \n",
      "8697  091809_w_43-BL1-097_bio.csv   \n",
      "8698  112016_m_25-BL1-091_bio.csv   \n",
      "8699  083013_w_47-BL1-086_bio.csv   \n",
      "\n",
      "                                            GSR signals  Pain level  \n",
      "5220  [0.0, 0.000784955924601847, 0.0012131137016623...           4  \n",
      "5221  [0.21654547886192313, 0.21654547886192313, 0.2...           4  \n",
      "5222  [1.0, 0.9983642739356123, 0.9979709555643436, ...           4  \n",
      "5223  [0.3629402970779174, 0.36178416384520784, 0.36...           4  \n",
      "5224  [0.16994520796566095, 0.1741950993228739, 0.17...           4  \n",
      "...                                                 ...         ...  \n",
      "8695  [0.9952579796341273, 0.9990653940310847, 1.0, ...           0  \n",
      "8696  [0.9974738284101647, 0.9974738284101757, 0.997...           0  \n",
      "8697  [0.9666771997357343, 0.9666771997356985, 0.966...           0  \n",
      "8698  [0.40274039309058246, 0.39576885709969134, 0.3...           0  \n",
      "8699  [0.984098311491783, 0.9840983114917017, 0.9840...           0  \n",
      "\n",
      "[3480 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([2784, 1, 2816])\n",
      "X_test_tensor shape: torch.Size([696, 1, 2816])\n",
      "y_train_tensor shape: torch.Size([2784])\n",
      "y_test_tensor shape: torch.Size([696])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Preprocess GSR signals\n",
    "def preprocess_gsr_signal(gsr_signal):\n",
    "    # Your preprocessing code here\n",
    "    return gsr_signal\n",
    "\n",
    "df['GSR signals'] = df['GSR signals'].apply(preprocess_gsr_signal)\n",
    "\n",
    "# Step 2: Pad or truncate signals\n",
    "max_length = max(len(signal) for signal in df['GSR signals'])\n",
    "gsr_signals = np.array([np.pad(signal, (0, max_length - len(signal))) if len(signal) < max_length else signal[:max_length] for signal in df['GSR signals']])\n",
    "\n",
    "# Step 3: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gsr_signals, df['Pain level'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Convert data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Assuming your labels are integer-encoded\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Check the shape of tensors\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([2784, 1, 2816])\n",
      "X_test_tensor shape: torch.Size([696, 1, 2816])\n",
      "y_train_tensor shape: torch.Size([2784])\n",
      "y_test_tensor shape: torch.Size([696])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the data\n",
    "max_length = max(len(signal) for signal in df['GSR signals'])  # Find the maximum length of GSR signals\n",
    "\n",
    "# Pad or truncate the GSR signals to the maximum length\n",
    "gsr_signals = np.array([np.pad(signal, (0, max_length - len(signal))) if len(signal) < max_length else signal[:max_length] for signal in df['GSR signals']])\n",
    "\n",
    "pain_levels = df['Pain level'].values\n",
    "\n",
    "# Step 2: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gsr_signals, pain_levels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  \n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Check the shape of tensors\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_tensor: The training data tensor with a shape of [6960, 1, 2816], indicating that there are 6960 samples, each with 1 channel (for the GSR signal), and each signal has been padded or truncated to a length of 2816.  \n",
    "X_test_tensor: The test data tensor with a shape of [1740, 1, 2816], indicating that there are 1740 samples in the test set, each with 1 channel, and the signals have the same length as the training data.  \n",
    "y_train_tensor: The training labels tensor with a shape of [6960], containing the corresponding pain levels for the training samples.  \n",
    "y_test_tensor: The test labels tensor with a shape of [1740], containing the corresponding pain levels for the test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension (29) represents the batch size, indicating that there are 29 samples in the batch.  \n",
    "The second dimension (1) represents the number of channels. In this case, there is only one channel.  \n",
    "The third dimension (1) represents the length of the input data for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data has a shape of (batch_size, channels, sequence_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model classes and utility functions\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network class to be used as teacher:\n",
    "\n",
    "class Conv1D_T(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Conv1D_T, self).__init__()\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(22336, 512)  \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Neural network class to be used as student:\n",
    "\n",
    "class Conv1D_S(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Conv1D_S, self).__init__()\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(22336, 512)  \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    # Define learning parameters\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 100\n",
    "    batch_size = 1024\n",
    "    num_classes = 2\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Initialize K-Fold cross-validation\n",
    "    k_folds = 5\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train_tensor)):\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "        \n",
    "        # Split data into training and validation sets for this fold\n",
    "        X_train_fold, X_val_fold = X_train_tensor[train_index], X_train_tensor[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_tensor[train_index], y_train_tensor[val_index]\n",
    "        \n",
    "        # Create DataLoader for training and validation sets\n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_accuracy = correct / total\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        y_test_tensor_sampled = y_test_tensor[:len(predictions)]\n",
    "        accuracy = torch.mean((predictions == y_test_tensor_sampled).float()).item()\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Calculate MAE and RMSE\n",
    "        mae = mean_absolute_error(y_test_tensor_sampled, predictions)\n",
    "        rmse = mean_squared_error(y_test_tensor_sampled, predictions, squared=False)\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "==================\n",
    "\n",
    "For reproducibility, we need to set the torch manual seed. I train\n",
    "networks using different methods, so to compare them fairly, it makes\n",
    "sense to initialize the networks with the same weights. I start by\n",
    "training the teacher network using cross-entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/100, Loss: 0.6974, Accuracy: 0.4946\n",
      "Epoch 2/100, Loss: 0.6942, Accuracy: 0.4932\n",
      "Epoch 3/100, Loss: 0.6941, Accuracy: 0.5068\n",
      "Epoch 4/100, Loss: 0.6943, Accuracy: 0.5041\n",
      "Epoch 5/100, Loss: 0.6933, Accuracy: 0.5156\n",
      "Epoch 6/100, Loss: 0.6931, Accuracy: 0.5009\n",
      "Epoch 7/100, Loss: 0.6933, Accuracy: 0.5063\n",
      "Epoch 8/100, Loss: 0.6931, Accuracy: 0.5022\n",
      "Epoch 9/100, Loss: 0.6929, Accuracy: 0.5072\n",
      "Epoch 10/100, Loss: 0.6929, Accuracy: 0.5153\n",
      "Epoch 11/100, Loss: 0.6928, Accuracy: 0.5165\n",
      "Epoch 12/100, Loss: 0.6926, Accuracy: 0.5138\n",
      "Epoch 13/100, Loss: 0.6926, Accuracy: 0.5084\n",
      "Epoch 14/100, Loss: 0.6925, Accuracy: 0.5074\n",
      "Epoch 15/100, Loss: 0.6924, Accuracy: 0.5154\n",
      "Epoch 16/100, Loss: 0.6923, Accuracy: 0.5174\n",
      "Epoch 17/100, Loss: 0.6922, Accuracy: 0.5183\n",
      "Epoch 18/100, Loss: 0.6921, Accuracy: 0.5198\n",
      "Epoch 19/100, Loss: 0.6921, Accuracy: 0.5217\n",
      "Epoch 20/100, Loss: 0.6920, Accuracy: 0.5207\n",
      "Epoch 21/100, Loss: 0.6919, Accuracy: 0.5198\n",
      "Epoch 22/100, Loss: 0.6918, Accuracy: 0.5208\n",
      "Epoch 23/100, Loss: 0.6918, Accuracy: 0.5156\n",
      "Epoch 24/100, Loss: 0.6917, Accuracy: 0.5178\n",
      "Epoch 25/100, Loss: 0.6916, Accuracy: 0.5187\n",
      "Epoch 26/100, Loss: 0.6915, Accuracy: 0.5262\n",
      "Epoch 27/100, Loss: 0.6914, Accuracy: 0.5304\n",
      "Epoch 28/100, Loss: 0.6913, Accuracy: 0.5304\n",
      "Epoch 29/100, Loss: 0.6912, Accuracy: 0.5305\n",
      "Epoch 30/100, Loss: 0.6912, Accuracy: 0.5221\n",
      "Epoch 31/100, Loss: 0.6911, Accuracy: 0.5221\n",
      "Epoch 32/100, Loss: 0.6911, Accuracy: 0.5275\n",
      "Epoch 33/100, Loss: 0.6909, Accuracy: 0.5368\n",
      "Epoch 34/100, Loss: 0.6908, Accuracy: 0.5357\n",
      "Epoch 35/100, Loss: 0.6908, Accuracy: 0.5268\n",
      "Epoch 36/100, Loss: 0.6907, Accuracy: 0.5287\n",
      "Epoch 37/100, Loss: 0.6906, Accuracy: 0.5361\n",
      "Epoch 38/100, Loss: 0.6905, Accuracy: 0.5383\n",
      "Epoch 39/100, Loss: 0.6904, Accuracy: 0.5411\n",
      "Epoch 40/100, Loss: 0.6904, Accuracy: 0.5388\n",
      "Epoch 41/100, Loss: 0.6903, Accuracy: 0.5409\n",
      "Epoch 42/100, Loss: 0.6902, Accuracy: 0.5404\n",
      "Epoch 43/100, Loss: 0.6901, Accuracy: 0.5388\n",
      "Epoch 44/100, Loss: 0.6900, Accuracy: 0.5350\n",
      "Epoch 45/100, Loss: 0.6900, Accuracy: 0.5422\n",
      "Epoch 46/100, Loss: 0.6899, Accuracy: 0.5401\n",
      "Epoch 47/100, Loss: 0.6898, Accuracy: 0.5402\n",
      "Epoch 48/100, Loss: 0.6897, Accuracy: 0.5454\n",
      "Epoch 49/100, Loss: 0.6896, Accuracy: 0.5474\n",
      "Epoch 50/100, Loss: 0.6895, Accuracy: 0.5480\n",
      "Epoch 51/100, Loss: 0.6894, Accuracy: 0.5463\n",
      "Epoch 52/100, Loss: 0.6894, Accuracy: 0.5418\n",
      "Epoch 53/100, Loss: 0.6893, Accuracy: 0.5497\n",
      "Epoch 54/100, Loss: 0.6892, Accuracy: 0.5533\n",
      "Epoch 55/100, Loss: 0.6891, Accuracy: 0.5535\n",
      "Epoch 56/100, Loss: 0.6890, Accuracy: 0.5557\n",
      "Epoch 57/100, Loss: 0.6889, Accuracy: 0.5560\n",
      "Epoch 58/100, Loss: 0.6889, Accuracy: 0.5541\n",
      "Epoch 59/100, Loss: 0.6888, Accuracy: 0.5587\n",
      "Epoch 60/100, Loss: 0.6887, Accuracy: 0.5548\n",
      "Epoch 61/100, Loss: 0.6886, Accuracy: 0.5505\n",
      "Epoch 62/100, Loss: 0.6885, Accuracy: 0.5560\n",
      "Epoch 63/100, Loss: 0.6884, Accuracy: 0.5620\n",
      "Epoch 64/100, Loss: 0.6884, Accuracy: 0.5603\n",
      "Epoch 65/100, Loss: 0.6884, Accuracy: 0.5594\n",
      "Epoch 66/100, Loss: 0.6882, Accuracy: 0.5632\n",
      "Epoch 67/100, Loss: 0.6881, Accuracy: 0.5623\n",
      "Epoch 68/100, Loss: 0.6881, Accuracy: 0.5577\n",
      "Epoch 69/100, Loss: 0.6881, Accuracy: 0.5600\n",
      "Epoch 70/100, Loss: 0.6879, Accuracy: 0.5673\n",
      "Epoch 71/100, Loss: 0.6878, Accuracy: 0.5650\n",
      "Epoch 72/100, Loss: 0.6877, Accuracy: 0.5684\n",
      "Epoch 73/100, Loss: 0.6876, Accuracy: 0.5715\n",
      "Epoch 74/100, Loss: 0.6876, Accuracy: 0.5691\n",
      "Epoch 75/100, Loss: 0.6875, Accuracy: 0.5672\n",
      "Epoch 76/100, Loss: 0.6874, Accuracy: 0.5717\n",
      "Epoch 77/100, Loss: 0.6873, Accuracy: 0.5688\n",
      "Epoch 78/100, Loss: 0.6873, Accuracy: 0.5656\n",
      "Epoch 79/100, Loss: 0.6871, Accuracy: 0.5722\n",
      "Epoch 80/100, Loss: 0.6870, Accuracy: 0.5754\n",
      "Epoch 81/100, Loss: 0.6871, Accuracy: 0.5708\n",
      "Epoch 82/100, Loss: 0.6869, Accuracy: 0.5778\n",
      "Epoch 83/100, Loss: 0.6869, Accuracy: 0.5677\n",
      "Epoch 84/100, Loss: 0.6868, Accuracy: 0.5706\n",
      "Epoch 85/100, Loss: 0.6867, Accuracy: 0.5765\n",
      "Epoch 86/100, Loss: 0.6866, Accuracy: 0.5745\n",
      "Epoch 87/100, Loss: 0.6865, Accuracy: 0.5742\n",
      "Epoch 88/100, Loss: 0.6864, Accuracy: 0.5767\n",
      "Epoch 89/100, Loss: 0.6864, Accuracy: 0.5801\n",
      "Epoch 90/100, Loss: 0.6863, Accuracy: 0.5760\n",
      "Epoch 91/100, Loss: 0.6862, Accuracy: 0.5814\n",
      "Epoch 92/100, Loss: 0.6861, Accuracy: 0.5830\n",
      "Epoch 93/100, Loss: 0.6860, Accuracy: 0.5851\n",
      "Epoch 94/100, Loss: 0.6859, Accuracy: 0.5855\n",
      "Epoch 95/100, Loss: 0.6858, Accuracy: 0.5828\n",
      "Epoch 96/100, Loss: 0.6858, Accuracy: 0.5884\n",
      "Epoch 97/100, Loss: 0.6857, Accuracy: 0.5839\n",
      "Epoch 98/100, Loss: 0.6856, Accuracy: 0.5857\n",
      "Epoch 99/100, Loss: 0.6855, Accuracy: 0.5875\n",
      "Epoch 100/100, Loss: 0.6854, Accuracy: 0.5869\n",
      "Fold 2/5\n",
      "Epoch 1/100, Loss: 0.6878, Accuracy: 0.5618\n",
      "Epoch 2/100, Loss: 0.6876, Accuracy: 0.5630\n",
      "Epoch 3/100, Loss: 0.6876, Accuracy: 0.5620\n",
      "Epoch 4/100, Loss: 0.6875, Accuracy: 0.5661\n",
      "Epoch 5/100, Loss: 0.6874, Accuracy: 0.5665\n",
      "Epoch 6/100, Loss: 0.6874, Accuracy: 0.5661\n",
      "Epoch 7/100, Loss: 0.6874, Accuracy: 0.5627\n",
      "Epoch 8/100, Loss: 0.6872, Accuracy: 0.5713\n",
      "Epoch 9/100, Loss: 0.6872, Accuracy: 0.5665\n",
      "Epoch 10/100, Loss: 0.6870, Accuracy: 0.5686\n",
      "Epoch 11/100, Loss: 0.6869, Accuracy: 0.5706\n",
      "Epoch 12/100, Loss: 0.6868, Accuracy: 0.5736\n",
      "Epoch 13/100, Loss: 0.6868, Accuracy: 0.5763\n",
      "Epoch 14/100, Loss: 0.6867, Accuracy: 0.5745\n",
      "Epoch 15/100, Loss: 0.6866, Accuracy: 0.5740\n",
      "Epoch 16/100, Loss: 0.6865, Accuracy: 0.5761\n",
      "Epoch 17/100, Loss: 0.6864, Accuracy: 0.5761\n",
      "Epoch 18/100, Loss: 0.6863, Accuracy: 0.5763\n",
      "Epoch 19/100, Loss: 0.6863, Accuracy: 0.5769\n",
      "Epoch 20/100, Loss: 0.6862, Accuracy: 0.5760\n",
      "Epoch 21/100, Loss: 0.6861, Accuracy: 0.5787\n",
      "Epoch 22/100, Loss: 0.6860, Accuracy: 0.5815\n",
      "Epoch 23/100, Loss: 0.6860, Accuracy: 0.5819\n",
      "Epoch 24/100, Loss: 0.6859, Accuracy: 0.5837\n",
      "Epoch 25/100, Loss: 0.6858, Accuracy: 0.5849\n",
      "Epoch 26/100, Loss: 0.6857, Accuracy: 0.5855\n",
      "Epoch 27/100, Loss: 0.6856, Accuracy: 0.5880\n",
      "Epoch 28/100, Loss: 0.6855, Accuracy: 0.5882\n",
      "Epoch 29/100, Loss: 0.6854, Accuracy: 0.5855\n",
      "Epoch 30/100, Loss: 0.6854, Accuracy: 0.5839\n",
      "Epoch 31/100, Loss: 0.6853, Accuracy: 0.5842\n",
      "Epoch 32/100, Loss: 0.6852, Accuracy: 0.5842\n",
      "Epoch 33/100, Loss: 0.6851, Accuracy: 0.5821\n",
      "Epoch 34/100, Loss: 0.6851, Accuracy: 0.5835\n",
      "Epoch 35/100, Loss: 0.6849, Accuracy: 0.5946\n",
      "Epoch 36/100, Loss: 0.6848, Accuracy: 0.5912\n",
      "Epoch 37/100, Loss: 0.6848, Accuracy: 0.5893\n",
      "Epoch 38/100, Loss: 0.6847, Accuracy: 0.5889\n",
      "Epoch 39/100, Loss: 0.6846, Accuracy: 0.5902\n",
      "Epoch 40/100, Loss: 0.6846, Accuracy: 0.5957\n",
      "Epoch 41/100, Loss: 0.6844, Accuracy: 0.5997\n",
      "Epoch 42/100, Loss: 0.6844, Accuracy: 0.5979\n",
      "Epoch 43/100, Loss: 0.6843, Accuracy: 0.5930\n",
      "Epoch 44/100, Loss: 0.6842, Accuracy: 0.5968\n",
      "Epoch 45/100, Loss: 0.6841, Accuracy: 0.5979\n",
      "Epoch 46/100, Loss: 0.6840, Accuracy: 0.5970\n",
      "Epoch 47/100, Loss: 0.6840, Accuracy: 0.5938\n",
      "Epoch 48/100, Loss: 0.6839, Accuracy: 0.5923\n",
      "Epoch 49/100, Loss: 0.6837, Accuracy: 0.5984\n",
      "Epoch 50/100, Loss: 0.6837, Accuracy: 0.6042\n",
      "Epoch 51/100, Loss: 0.6836, Accuracy: 0.6034\n",
      "Epoch 52/100, Loss: 0.6835, Accuracy: 0.5973\n",
      "Epoch 53/100, Loss: 0.6834, Accuracy: 0.5975\n",
      "Epoch 54/100, Loss: 0.6834, Accuracy: 0.6020\n",
      "Epoch 55/100, Loss: 0.6832, Accuracy: 0.6061\n",
      "Epoch 56/100, Loss: 0.6832, Accuracy: 0.6094\n",
      "Epoch 57/100, Loss: 0.6831, Accuracy: 0.6096\n",
      "Epoch 58/100, Loss: 0.6830, Accuracy: 0.6072\n",
      "Epoch 59/100, Loss: 0.6829, Accuracy: 0.6065\n",
      "Epoch 60/100, Loss: 0.6829, Accuracy: 0.6047\n",
      "Epoch 61/100, Loss: 0.6828, Accuracy: 0.6051\n",
      "Epoch 62/100, Loss: 0.6826, Accuracy: 0.6078\n",
      "Epoch 63/100, Loss: 0.6826, Accuracy: 0.6124\n",
      "Epoch 64/100, Loss: 0.6825, Accuracy: 0.6131\n",
      "Epoch 65/100, Loss: 0.6824, Accuracy: 0.6149\n",
      "Epoch 66/100, Loss: 0.6824, Accuracy: 0.6114\n",
      "Epoch 67/100, Loss: 0.6823, Accuracy: 0.6092\n",
      "Epoch 68/100, Loss: 0.6821, Accuracy: 0.6144\n",
      "Epoch 69/100, Loss: 0.6821, Accuracy: 0.6164\n",
      "Epoch 70/100, Loss: 0.6820, Accuracy: 0.6158\n",
      "Epoch 71/100, Loss: 0.6819, Accuracy: 0.6169\n",
      "Epoch 72/100, Loss: 0.6818, Accuracy: 0.6160\n",
      "Epoch 73/100, Loss: 0.6817, Accuracy: 0.6164\n",
      "Epoch 74/100, Loss: 0.6816, Accuracy: 0.6212\n",
      "Epoch 75/100, Loss: 0.6816, Accuracy: 0.6209\n",
      "Epoch 76/100, Loss: 0.6815, Accuracy: 0.6184\n",
      "Epoch 77/100, Loss: 0.6814, Accuracy: 0.6155\n",
      "Epoch 78/100, Loss: 0.6813, Accuracy: 0.6184\n",
      "Epoch 79/100, Loss: 0.6812, Accuracy: 0.6219\n",
      "Epoch 80/100, Loss: 0.6811, Accuracy: 0.6214\n",
      "Epoch 81/100, Loss: 0.6811, Accuracy: 0.6250\n",
      "Epoch 82/100, Loss: 0.6810, Accuracy: 0.6209\n",
      "Epoch 83/100, Loss: 0.6809, Accuracy: 0.6198\n",
      "Epoch 84/100, Loss: 0.6808, Accuracy: 0.6225\n",
      "Epoch 85/100, Loss: 0.6807, Accuracy: 0.6281\n",
      "Epoch 86/100, Loss: 0.6806, Accuracy: 0.6279\n",
      "Epoch 87/100, Loss: 0.6805, Accuracy: 0.6254\n",
      "Epoch 88/100, Loss: 0.6805, Accuracy: 0.6218\n",
      "Epoch 89/100, Loss: 0.6804, Accuracy: 0.6228\n",
      "Epoch 90/100, Loss: 0.6803, Accuracy: 0.6286\n",
      "Epoch 91/100, Loss: 0.6802, Accuracy: 0.6320\n",
      "Epoch 92/100, Loss: 0.6802, Accuracy: 0.6279\n",
      "Epoch 93/100, Loss: 0.6800, Accuracy: 0.6281\n",
      "Epoch 94/100, Loss: 0.6800, Accuracy: 0.6282\n",
      "Epoch 95/100, Loss: 0.6798, Accuracy: 0.6327\n",
      "Epoch 96/100, Loss: 0.6798, Accuracy: 0.6361\n",
      "Epoch 97/100, Loss: 0.6798, Accuracy: 0.6367\n",
      "Epoch 98/100, Loss: 0.6796, Accuracy: 0.6403\n",
      "Epoch 99/100, Loss: 0.6795, Accuracy: 0.6325\n",
      "Epoch 100/100, Loss: 0.6794, Accuracy: 0.6298\n",
      "Fold 3/5\n",
      "Epoch 1/100, Loss: 0.6815, Accuracy: 0.6178\n",
      "Epoch 2/100, Loss: 0.6815, Accuracy: 0.6187\n",
      "Epoch 3/100, Loss: 0.6813, Accuracy: 0.6114\n",
      "Epoch 4/100, Loss: 0.6811, Accuracy: 0.6194\n",
      "Epoch 5/100, Loss: 0.6811, Accuracy: 0.6234\n",
      "Epoch 6/100, Loss: 0.6810, Accuracy: 0.6239\n",
      "Epoch 7/100, Loss: 0.6809, Accuracy: 0.6259\n",
      "Epoch 8/100, Loss: 0.6807, Accuracy: 0.6257\n",
      "Epoch 9/100, Loss: 0.6807, Accuracy: 0.6209\n",
      "Epoch 10/100, Loss: 0.6806, Accuracy: 0.6212\n",
      "Epoch 11/100, Loss: 0.6805, Accuracy: 0.6236\n",
      "Epoch 12/100, Loss: 0.6804, Accuracy: 0.6320\n",
      "Epoch 13/100, Loss: 0.6803, Accuracy: 0.6304\n",
      "Epoch 14/100, Loss: 0.6802, Accuracy: 0.6297\n",
      "Epoch 15/100, Loss: 0.6802, Accuracy: 0.6309\n",
      "Epoch 16/100, Loss: 0.6801, Accuracy: 0.6316\n",
      "Epoch 17/100, Loss: 0.6800, Accuracy: 0.6329\n",
      "Epoch 18/100, Loss: 0.6800, Accuracy: 0.6336\n",
      "Epoch 19/100, Loss: 0.6798, Accuracy: 0.6331\n",
      "Epoch 20/100, Loss: 0.6798, Accuracy: 0.6367\n",
      "Epoch 21/100, Loss: 0.6797, Accuracy: 0.6352\n",
      "Epoch 22/100, Loss: 0.6795, Accuracy: 0.6363\n",
      "Epoch 23/100, Loss: 0.6795, Accuracy: 0.6343\n",
      "Epoch 24/100, Loss: 0.6794, Accuracy: 0.6324\n",
      "Epoch 25/100, Loss: 0.6794, Accuracy: 0.6378\n",
      "Epoch 26/100, Loss: 0.6792, Accuracy: 0.6374\n",
      "Epoch 27/100, Loss: 0.6792, Accuracy: 0.6349\n",
      "Epoch 28/100, Loss: 0.6790, Accuracy: 0.6392\n",
      "Epoch 29/100, Loss: 0.6790, Accuracy: 0.6378\n",
      "Epoch 30/100, Loss: 0.6789, Accuracy: 0.6338\n",
      "Epoch 31/100, Loss: 0.6788, Accuracy: 0.6381\n",
      "Epoch 32/100, Loss: 0.6787, Accuracy: 0.6437\n",
      "Epoch 33/100, Loss: 0.6786, Accuracy: 0.6433\n",
      "Epoch 34/100, Loss: 0.6785, Accuracy: 0.6442\n",
      "Epoch 35/100, Loss: 0.6785, Accuracy: 0.6412\n",
      "Epoch 36/100, Loss: 0.6784, Accuracy: 0.6460\n",
      "Epoch 37/100, Loss: 0.6783, Accuracy: 0.6483\n",
      "Epoch 38/100, Loss: 0.6782, Accuracy: 0.6462\n",
      "Epoch 39/100, Loss: 0.6781, Accuracy: 0.6471\n",
      "Epoch 40/100, Loss: 0.6780, Accuracy: 0.6478\n",
      "Epoch 41/100, Loss: 0.6779, Accuracy: 0.6466\n",
      "Epoch 42/100, Loss: 0.6778, Accuracy: 0.6514\n",
      "Epoch 43/100, Loss: 0.6777, Accuracy: 0.6457\n",
      "Epoch 44/100, Loss: 0.6776, Accuracy: 0.6492\n",
      "Epoch 45/100, Loss: 0.6776, Accuracy: 0.6507\n",
      "Epoch 46/100, Loss: 0.6776, Accuracy: 0.6505\n",
      "Epoch 47/100, Loss: 0.6774, Accuracy: 0.6503\n",
      "Epoch 48/100, Loss: 0.6772, Accuracy: 0.6543\n",
      "Epoch 49/100, Loss: 0.6774, Accuracy: 0.6352\n",
      "Epoch 50/100, Loss: 0.6772, Accuracy: 0.6437\n",
      "Epoch 51/100, Loss: 0.6770, Accuracy: 0.6541\n",
      "Epoch 52/100, Loss: 0.6769, Accuracy: 0.6550\n",
      "Epoch 53/100, Loss: 0.6769, Accuracy: 0.6559\n",
      "Epoch 54/100, Loss: 0.6768, Accuracy: 0.6570\n",
      "Epoch 55/100, Loss: 0.6767, Accuracy: 0.6557\n",
      "Epoch 56/100, Loss: 0.6766, Accuracy: 0.6528\n",
      "Epoch 57/100, Loss: 0.6765, Accuracy: 0.6546\n",
      "Epoch 58/100, Loss: 0.6764, Accuracy: 0.6570\n",
      "Epoch 59/100, Loss: 0.6763, Accuracy: 0.6598\n",
      "Epoch 60/100, Loss: 0.6763, Accuracy: 0.6554\n",
      "Epoch 61/100, Loss: 0.6761, Accuracy: 0.6570\n",
      "Epoch 62/100, Loss: 0.6760, Accuracy: 0.6607\n",
      "Epoch 63/100, Loss: 0.6760, Accuracy: 0.6606\n",
      "Epoch 64/100, Loss: 0.6759, Accuracy: 0.6616\n",
      "Epoch 65/100, Loss: 0.6758, Accuracy: 0.6566\n",
      "Epoch 66/100, Loss: 0.6757, Accuracy: 0.6627\n",
      "Epoch 67/100, Loss: 0.6756, Accuracy: 0.6654\n",
      "Epoch 68/100, Loss: 0.6755, Accuracy: 0.6631\n",
      "Epoch 69/100, Loss: 0.6754, Accuracy: 0.6624\n",
      "Epoch 70/100, Loss: 0.6754, Accuracy: 0.6627\n",
      "Epoch 71/100, Loss: 0.6752, Accuracy: 0.6685\n",
      "Epoch 72/100, Loss: 0.6752, Accuracy: 0.6611\n",
      "Epoch 73/100, Loss: 0.6751, Accuracy: 0.6609\n",
      "Epoch 74/100, Loss: 0.6750, Accuracy: 0.6645\n",
      "Epoch 75/100, Loss: 0.6749, Accuracy: 0.6694\n",
      "Epoch 76/100, Loss: 0.6748, Accuracy: 0.6695\n",
      "Epoch 77/100, Loss: 0.6747, Accuracy: 0.6683\n",
      "Epoch 78/100, Loss: 0.6746, Accuracy: 0.6717\n",
      "Epoch 79/100, Loss: 0.6745, Accuracy: 0.6685\n",
      "Epoch 80/100, Loss: 0.6744, Accuracy: 0.6722\n",
      "Epoch 81/100, Loss: 0.6744, Accuracy: 0.6710\n",
      "Epoch 82/100, Loss: 0.6743, Accuracy: 0.6683\n",
      "Epoch 83/100, Loss: 0.6741, Accuracy: 0.6699\n",
      "Epoch 84/100, Loss: 0.6741, Accuracy: 0.6712\n",
      "Epoch 85/100, Loss: 0.6739, Accuracy: 0.6710\n",
      "Epoch 86/100, Loss: 0.6739, Accuracy: 0.6710\n",
      "Epoch 87/100, Loss: 0.6738, Accuracy: 0.6728\n",
      "Epoch 88/100, Loss: 0.6737, Accuracy: 0.6749\n",
      "Epoch 89/100, Loss: 0.6736, Accuracy: 0.6739\n",
      "Epoch 90/100, Loss: 0.6735, Accuracy: 0.6731\n",
      "Epoch 91/100, Loss: 0.6734, Accuracy: 0.6756\n",
      "Epoch 92/100, Loss: 0.6733, Accuracy: 0.6805\n",
      "Epoch 93/100, Loss: 0.6733, Accuracy: 0.6704\n",
      "Epoch 94/100, Loss: 0.6731, Accuracy: 0.6733\n",
      "Epoch 95/100, Loss: 0.6730, Accuracy: 0.6776\n",
      "Epoch 96/100, Loss: 0.6730, Accuracy: 0.6760\n",
      "Epoch 97/100, Loss: 0.6728, Accuracy: 0.6785\n",
      "Epoch 98/100, Loss: 0.6728, Accuracy: 0.6796\n",
      "Epoch 99/100, Loss: 0.6728, Accuracy: 0.6674\n",
      "Epoch 100/100, Loss: 0.6726, Accuracy: 0.6713\n",
      "Fold 4/5\n",
      "Epoch 1/100, Loss: 0.6744, Accuracy: 0.6625\n",
      "Epoch 2/100, Loss: 0.6744, Accuracy: 0.6579\n",
      "Epoch 3/100, Loss: 0.6743, Accuracy: 0.6598\n",
      "Epoch 4/100, Loss: 0.6742, Accuracy: 0.6622\n",
      "Epoch 5/100, Loss: 0.6741, Accuracy: 0.6654\n",
      "Epoch 6/100, Loss: 0.6740, Accuracy: 0.6665\n",
      "Epoch 7/100, Loss: 0.6739, Accuracy: 0.6634\n",
      "Epoch 8/100, Loss: 0.6739, Accuracy: 0.6582\n",
      "Epoch 9/100, Loss: 0.6737, Accuracy: 0.6609\n",
      "Epoch 10/100, Loss: 0.6736, Accuracy: 0.6676\n",
      "Epoch 11/100, Loss: 0.6735, Accuracy: 0.6699\n",
      "Epoch 12/100, Loss: 0.6734, Accuracy: 0.6685\n",
      "Epoch 13/100, Loss: 0.6733, Accuracy: 0.6694\n",
      "Epoch 14/100, Loss: 0.6732, Accuracy: 0.6710\n",
      "Epoch 15/100, Loss: 0.6731, Accuracy: 0.6701\n",
      "Epoch 16/100, Loss: 0.6730, Accuracy: 0.6712\n",
      "Epoch 17/100, Loss: 0.6729, Accuracy: 0.6710\n",
      "Epoch 18/100, Loss: 0.6728, Accuracy: 0.6715\n",
      "Epoch 19/100, Loss: 0.6727, Accuracy: 0.6717\n",
      "Epoch 20/100, Loss: 0.6727, Accuracy: 0.6724\n",
      "Epoch 21/100, Loss: 0.6726, Accuracy: 0.6746\n",
      "Epoch 22/100, Loss: 0.6725, Accuracy: 0.6713\n",
      "Epoch 23/100, Loss: 0.6723, Accuracy: 0.6742\n",
      "Epoch 24/100, Loss: 0.6723, Accuracy: 0.6731\n",
      "Epoch 25/100, Loss: 0.6723, Accuracy: 0.6749\n",
      "Epoch 26/100, Loss: 0.6721, Accuracy: 0.6744\n",
      "Epoch 27/100, Loss: 0.6720, Accuracy: 0.6774\n",
      "Epoch 28/100, Loss: 0.6720, Accuracy: 0.6776\n",
      "Epoch 29/100, Loss: 0.6718, Accuracy: 0.6774\n",
      "Epoch 30/100, Loss: 0.6718, Accuracy: 0.6739\n",
      "Epoch 31/100, Loss: 0.6716, Accuracy: 0.6764\n",
      "Epoch 32/100, Loss: 0.6715, Accuracy: 0.6805\n",
      "Epoch 33/100, Loss: 0.6714, Accuracy: 0.6785\n",
      "Epoch 34/100, Loss: 0.6713, Accuracy: 0.6805\n",
      "Epoch 35/100, Loss: 0.6712, Accuracy: 0.6814\n",
      "Epoch 36/100, Loss: 0.6711, Accuracy: 0.6827\n",
      "Epoch 37/100, Loss: 0.6710, Accuracy: 0.6828\n",
      "Epoch 38/100, Loss: 0.6709, Accuracy: 0.6841\n",
      "Epoch 39/100, Loss: 0.6708, Accuracy: 0.6832\n",
      "Epoch 40/100, Loss: 0.6707, Accuracy: 0.6821\n",
      "Epoch 41/100, Loss: 0.6706, Accuracy: 0.6857\n",
      "Epoch 42/100, Loss: 0.6706, Accuracy: 0.6832\n",
      "Epoch 43/100, Loss: 0.6705, Accuracy: 0.6828\n",
      "Epoch 44/100, Loss: 0.6703, Accuracy: 0.6871\n",
      "Epoch 45/100, Loss: 0.6703, Accuracy: 0.6841\n",
      "Epoch 46/100, Loss: 0.6702, Accuracy: 0.6837\n",
      "Epoch 47/100, Loss: 0.6701, Accuracy: 0.6870\n",
      "Epoch 48/100, Loss: 0.6700, Accuracy: 0.6862\n",
      "Epoch 49/100, Loss: 0.6699, Accuracy: 0.6868\n",
      "Epoch 50/100, Loss: 0.6698, Accuracy: 0.6861\n",
      "Epoch 51/100, Loss: 0.6698, Accuracy: 0.6830\n",
      "Epoch 52/100, Loss: 0.6696, Accuracy: 0.6825\n",
      "Epoch 53/100, Loss: 0.6695, Accuracy: 0.6884\n",
      "Epoch 54/100, Loss: 0.6694, Accuracy: 0.6906\n",
      "Epoch 55/100, Loss: 0.6693, Accuracy: 0.6870\n",
      "Epoch 56/100, Loss: 0.6693, Accuracy: 0.6880\n",
      "Epoch 57/100, Loss: 0.6691, Accuracy: 0.6888\n",
      "Epoch 58/100, Loss: 0.6690, Accuracy: 0.6880\n",
      "Epoch 59/100, Loss: 0.6688, Accuracy: 0.6900\n",
      "Epoch 60/100, Loss: 0.6688, Accuracy: 0.6915\n",
      "Epoch 61/100, Loss: 0.6687, Accuracy: 0.6916\n",
      "Epoch 62/100, Loss: 0.6686, Accuracy: 0.6907\n",
      "Epoch 63/100, Loss: 0.6685, Accuracy: 0.6941\n",
      "Epoch 64/100, Loss: 0.6684, Accuracy: 0.6925\n",
      "Epoch 65/100, Loss: 0.6683, Accuracy: 0.6923\n",
      "Epoch 66/100, Loss: 0.6682, Accuracy: 0.6934\n",
      "Epoch 67/100, Loss: 0.6681, Accuracy: 0.6907\n",
      "Epoch 68/100, Loss: 0.6680, Accuracy: 0.6927\n",
      "Epoch 69/100, Loss: 0.6679, Accuracy: 0.6909\n",
      "Epoch 70/100, Loss: 0.6678, Accuracy: 0.6952\n",
      "Epoch 71/100, Loss: 0.6677, Accuracy: 0.6961\n",
      "Epoch 72/100, Loss: 0.6676, Accuracy: 0.6936\n",
      "Epoch 73/100, Loss: 0.6675, Accuracy: 0.6974\n",
      "Epoch 74/100, Loss: 0.6674, Accuracy: 0.6952\n",
      "Epoch 75/100, Loss: 0.6673, Accuracy: 0.6952\n",
      "Epoch 76/100, Loss: 0.6672, Accuracy: 0.6977\n",
      "Epoch 77/100, Loss: 0.6671, Accuracy: 0.6985\n",
      "Epoch 78/100, Loss: 0.6669, Accuracy: 0.6988\n",
      "Epoch 79/100, Loss: 0.6668, Accuracy: 0.6985\n",
      "Epoch 80/100, Loss: 0.6668, Accuracy: 0.6995\n",
      "Epoch 81/100, Loss: 0.6667, Accuracy: 0.6997\n",
      "Epoch 82/100, Loss: 0.6666, Accuracy: 0.7026\n",
      "Epoch 83/100, Loss: 0.6664, Accuracy: 0.6985\n",
      "Epoch 84/100, Loss: 0.6663, Accuracy: 0.6976\n",
      "Epoch 85/100, Loss: 0.6663, Accuracy: 0.6997\n",
      "Epoch 86/100, Loss: 0.6661, Accuracy: 0.7024\n",
      "Epoch 87/100, Loss: 0.6660, Accuracy: 0.7010\n",
      "Epoch 88/100, Loss: 0.6659, Accuracy: 0.6997\n",
      "Epoch 89/100, Loss: 0.6658, Accuracy: 0.7003\n",
      "Epoch 90/100, Loss: 0.6657, Accuracy: 0.6999\n",
      "Epoch 91/100, Loss: 0.6657, Accuracy: 0.7020\n",
      "Epoch 92/100, Loss: 0.6655, Accuracy: 0.7028\n",
      "Epoch 93/100, Loss: 0.6655, Accuracy: 0.7022\n",
      "Epoch 94/100, Loss: 0.6654, Accuracy: 0.7019\n",
      "Epoch 95/100, Loss: 0.6652, Accuracy: 0.7058\n",
      "Epoch 96/100, Loss: 0.6651, Accuracy: 0.7042\n",
      "Epoch 97/100, Loss: 0.6650, Accuracy: 0.7051\n",
      "Epoch 98/100, Loss: 0.6649, Accuracy: 0.7044\n",
      "Epoch 99/100, Loss: 0.6649, Accuracy: 0.7004\n",
      "Epoch 100/100, Loss: 0.6647, Accuracy: 0.7042\n",
      "Fold 5/5\n",
      "Epoch 1/100, Loss: 0.6675, Accuracy: 0.6920\n",
      "Epoch 2/100, Loss: 0.6675, Accuracy: 0.6907\n",
      "Epoch 3/100, Loss: 0.6674, Accuracy: 0.6925\n",
      "Epoch 4/100, Loss: 0.6673, Accuracy: 0.6906\n",
      "Epoch 5/100, Loss: 0.6672, Accuracy: 0.6943\n",
      "Epoch 6/100, Loss: 0.6671, Accuracy: 0.6900\n",
      "Epoch 7/100, Loss: 0.6670, Accuracy: 0.6916\n",
      "Epoch 8/100, Loss: 0.6669, Accuracy: 0.6907\n",
      "Epoch 9/100, Loss: 0.6667, Accuracy: 0.6954\n",
      "Epoch 10/100, Loss: 0.6667, Accuracy: 0.6945\n",
      "Epoch 11/100, Loss: 0.6666, Accuracy: 0.6913\n",
      "Epoch 12/100, Loss: 0.6664, Accuracy: 0.6958\n",
      "Epoch 13/100, Loss: 0.6664, Accuracy: 0.6977\n",
      "Epoch 14/100, Loss: 0.6662, Accuracy: 0.6970\n",
      "Epoch 15/100, Loss: 0.6661, Accuracy: 0.6979\n",
      "Epoch 16/100, Loss: 0.6660, Accuracy: 0.6986\n",
      "Epoch 17/100, Loss: 0.6659, Accuracy: 0.6972\n",
      "Epoch 18/100, Loss: 0.6658, Accuracy: 0.6970\n",
      "Epoch 19/100, Loss: 0.6657, Accuracy: 0.6992\n",
      "Epoch 20/100, Loss: 0.6656, Accuracy: 0.6961\n",
      "Epoch 21/100, Loss: 0.6656, Accuracy: 0.6923\n",
      "Epoch 22/100, Loss: 0.6654, Accuracy: 0.6929\n",
      "Epoch 23/100, Loss: 0.6653, Accuracy: 0.6992\n",
      "Epoch 24/100, Loss: 0.6651, Accuracy: 0.6985\n",
      "Epoch 25/100, Loss: 0.6651, Accuracy: 0.6983\n",
      "Epoch 26/100, Loss: 0.6650, Accuracy: 0.7019\n",
      "Epoch 27/100, Loss: 0.6649, Accuracy: 0.7008\n",
      "Epoch 28/100, Loss: 0.6647, Accuracy: 0.7020\n",
      "Epoch 29/100, Loss: 0.6646, Accuracy: 0.7015\n",
      "Epoch 30/100, Loss: 0.6645, Accuracy: 0.6997\n",
      "Epoch 31/100, Loss: 0.6644, Accuracy: 0.7013\n",
      "Epoch 32/100, Loss: 0.6643, Accuracy: 0.7028\n",
      "Epoch 33/100, Loss: 0.6642, Accuracy: 0.7035\n",
      "Epoch 34/100, Loss: 0.6641, Accuracy: 0.7015\n",
      "Epoch 35/100, Loss: 0.6640, Accuracy: 0.7049\n",
      "Epoch 36/100, Loss: 0.6639, Accuracy: 0.7044\n",
      "Epoch 37/100, Loss: 0.6638, Accuracy: 0.7035\n",
      "Epoch 38/100, Loss: 0.6637, Accuracy: 0.7035\n",
      "Epoch 39/100, Loss: 0.6635, Accuracy: 0.7049\n",
      "Epoch 40/100, Loss: 0.6634, Accuracy: 0.7055\n",
      "Epoch 41/100, Loss: 0.6633, Accuracy: 0.7047\n",
      "Epoch 42/100, Loss: 0.6633, Accuracy: 0.7055\n",
      "Epoch 43/100, Loss: 0.6631, Accuracy: 0.7053\n",
      "Epoch 44/100, Loss: 0.6630, Accuracy: 0.7073\n",
      "Epoch 45/100, Loss: 0.6629, Accuracy: 0.7064\n",
      "Epoch 46/100, Loss: 0.6628, Accuracy: 0.7062\n",
      "Epoch 47/100, Loss: 0.6626, Accuracy: 0.7069\n",
      "Epoch 48/100, Loss: 0.6626, Accuracy: 0.7053\n",
      "Epoch 49/100, Loss: 0.6624, Accuracy: 0.7073\n",
      "Epoch 50/100, Loss: 0.6623, Accuracy: 0.7078\n",
      "Epoch 51/100, Loss: 0.6622, Accuracy: 0.7058\n",
      "Epoch 52/100, Loss: 0.6621, Accuracy: 0.7076\n",
      "Epoch 53/100, Loss: 0.6620, Accuracy: 0.7091\n",
      "Epoch 54/100, Loss: 0.6619, Accuracy: 0.7089\n",
      "Epoch 55/100, Loss: 0.6618, Accuracy: 0.7091\n",
      "Epoch 56/100, Loss: 0.6617, Accuracy: 0.7083\n",
      "Epoch 57/100, Loss: 0.6616, Accuracy: 0.7083\n",
      "Epoch 58/100, Loss: 0.6615, Accuracy: 0.7094\n",
      "Epoch 59/100, Loss: 0.6613, Accuracy: 0.7103\n",
      "Epoch 60/100, Loss: 0.6613, Accuracy: 0.7060\n",
      "Epoch 61/100, Loss: 0.6611, Accuracy: 0.7101\n",
      "Epoch 62/100, Loss: 0.6610, Accuracy: 0.7114\n",
      "Epoch 63/100, Loss: 0.6609, Accuracy: 0.7116\n",
      "Epoch 64/100, Loss: 0.6607, Accuracy: 0.7119\n",
      "Epoch 65/100, Loss: 0.6606, Accuracy: 0.7128\n",
      "Epoch 66/100, Loss: 0.6605, Accuracy: 0.7141\n",
      "Epoch 67/100, Loss: 0.6604, Accuracy: 0.7110\n",
      "Epoch 68/100, Loss: 0.6603, Accuracy: 0.7125\n",
      "Epoch 69/100, Loss: 0.6602, Accuracy: 0.7137\n",
      "Epoch 70/100, Loss: 0.6601, Accuracy: 0.7157\n",
      "Epoch 71/100, Loss: 0.6600, Accuracy: 0.7141\n",
      "Epoch 72/100, Loss: 0.6599, Accuracy: 0.7144\n",
      "Epoch 73/100, Loss: 0.6597, Accuracy: 0.7130\n",
      "Epoch 74/100, Loss: 0.6596, Accuracy: 0.7139\n",
      "Epoch 75/100, Loss: 0.6595, Accuracy: 0.7139\n",
      "Epoch 76/100, Loss: 0.6594, Accuracy: 0.7143\n",
      "Epoch 77/100, Loss: 0.6593, Accuracy: 0.7168\n",
      "Epoch 78/100, Loss: 0.6592, Accuracy: 0.7153\n",
      "Epoch 79/100, Loss: 0.6590, Accuracy: 0.7150\n",
      "Epoch 80/100, Loss: 0.6589, Accuracy: 0.7162\n",
      "Epoch 81/100, Loss: 0.6588, Accuracy: 0.7144\n",
      "Epoch 82/100, Loss: 0.6587, Accuracy: 0.7184\n",
      "Epoch 83/100, Loss: 0.6585, Accuracy: 0.7162\n",
      "Epoch 84/100, Loss: 0.6585, Accuracy: 0.7159\n",
      "Epoch 85/100, Loss: 0.6583, Accuracy: 0.7189\n",
      "Epoch 86/100, Loss: 0.6582, Accuracy: 0.7175\n",
      "Epoch 87/100, Loss: 0.6581, Accuracy: 0.7204\n",
      "Epoch 88/100, Loss: 0.6580, Accuracy: 0.7191\n",
      "Epoch 89/100, Loss: 0.6578, Accuracy: 0.7195\n",
      "Epoch 90/100, Loss: 0.6578, Accuracy: 0.7182\n",
      "Epoch 91/100, Loss: 0.6576, Accuracy: 0.7189\n",
      "Epoch 92/100, Loss: 0.6575, Accuracy: 0.7195\n",
      "Epoch 93/100, Loss: 0.6574, Accuracy: 0.7202\n",
      "Epoch 94/100, Loss: 0.6573, Accuracy: 0.7205\n",
      "Epoch 95/100, Loss: 0.6571, Accuracy: 0.7200\n",
      "Epoch 96/100, Loss: 0.6570, Accuracy: 0.7216\n",
      "Epoch 97/100, Loss: 0.6569, Accuracy: 0.7216\n",
      "Epoch 98/100, Loss: 0.6568, Accuracy: 0.7220\n",
      "Epoch 99/100, Loss: 0.6567, Accuracy: 0.7216\n",
      "Epoch 100/100, Loss: 0.6565, Accuracy: 0.7218\n",
      "Test Accuracy: 0.4868\n",
      "MAE: 0.5132\n",
      "RMSE: 0.7164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ens/AU59350/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "# Define training and testing datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the teacher network\n",
    "torch.manual_seed(42)\n",
    "nn_t = Conv1D_T(num_classes=2).to(device)\n",
    "train(nn_t, train_loader, epochs=100, learning_rate=0.0001, device=device)\n",
    "\n",
    "# Test the teacher network\n",
    "test_accuracy_T = test(nn_t, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network\n",
    "torch.manual_seed(42)\n",
    "nn_s = Conv1D_S(num_classes=2).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I instantiate one more lightweight network model to compare their\n",
    "performances. Back propagation is sensitive to weight initialization, so\n",
    "I need to make sure these two networks have the exact same\n",
    "initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_Conv1D_S = Conv1D_S(num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure I have created a copy of the first network, we inspect the\n",
    "norm of its first layer. If it matches, then the networks are indeed the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(Conv1D_S.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_Conv1D_S.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of parameters in each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_T = \"{:,}\".format(sum(p.numel() for p in Conv1D_T.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_T}\")\n",
    "total_params_S = \"{:,}\".format(sum(p.numel() for p in Conv1D_S.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the lightweight network with cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(Conv1D_S, train_loader, epochs=10, learning_rate=0.0001, device=device)\n",
    "test_accuracy_S_ce = test(Conv1D_S, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on test accuracy, I can now compare the deeper\n",
    "network that is to be used as a teacher with the lightweight network\n",
    "that is the supposed student. So far, the student has not intervened\n",
    "with the teacher, therefore this performance is achieved by the student\n",
    "itself. The metrics so far can be seen with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_T:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_S_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    # Define learning parameters\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(student.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=Conv1D_T, student=new_Conv1D_S, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_T:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_S_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
