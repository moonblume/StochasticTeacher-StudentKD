{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation \n",
    "===============================\n",
    "\n",
    "**Author**: [Clara Martinez](https://github.com/moonblume/LIVIA.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique that enables knowledge transfer\n",
    "from large, computationally expensive models to smaller ones without\n",
    "losing validity. This allows for deployment on less powerful hardware,\n",
    "making evaluation faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librairies\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from typing import List, Union, Tuple, Any\n",
    "import statistics\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset\n",
    "================\n",
    "\n",
    "The GSRDataset class is a custom dataset class that inherits from PyTorch's Dataset class. It is designed to handle and preprocess physiological data, specifically Galvanic Skin Response (GSR) signals, along with their corresponding labels from video recordings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GSRDataset(Dataset):\n",
    "    def __init__(self, annotationfile_path, biosignals_path):\n",
    "        self.biosignals_path = biosignals_path\n",
    "        self.annotationfile_path = annotationfile_path\n",
    "        self._parse_annotationfile()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.data[index]['label']\n",
    "        gsr_data = self._load_biosignals(self.data[index]['path'])\n",
    "        gsr_data = torch.tensor(gsr_data.values, dtype=torch.float32)\n",
    "        return gsr_data, label\n",
    "\n",
    "    def _load_biosignals(self, filepath):\n",
    "        csv_path = os.path.join(self.biosignals_path, filepath)\n",
    "        physio_df = pd.read_csv(csv_path, sep='\\t')\n",
    "        gsr_data = physio_df['gsr']\n",
    "        return gsr_data\n",
    "\n",
    "    def _parse_annotationfile(self):\n",
    "        self.data = []\n",
    "        with open(self.annotationfile_path, 'r') as file:\n",
    "            for line in file:\n",
    "                path, label = line.strip().split()\n",
    "                self.data.append({'path': path, 'label': int(label)})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2816,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_sample_gsr_data(biosignals_path, sample_file):\n",
    "    csv_path = os.path.join(biosignals_path, sample_file)\n",
    "    physio_df = pd.read_csv(csv_path, sep='\\t')\n",
    "    gsr_data = physio_df['gsr']\n",
    "    return gsr_data\n",
    "\n",
    "# Replace these with actual paths\n",
    "biosignals_path = '/projets2/AS84330/Datasets/Biovid/PartA/physio/physio_organised/0'\n",
    "sample_file = '071309_w_21-BL1-081_bio.csv'\n",
    "\n",
    "gsr_data = load_sample_gsr_data(biosignals_path, sample_file)\n",
    "print(gsr_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSR data shape: (2816,)\n",
      "Number of samples (length of GSR signal): 2816\n"
     ]
    }
   ],
   "source": [
    "print(f'GSR data shape: {gsr_data.shape}')\n",
    "print(f'Number of samples (length of GSR signal): {len(gsr_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Conv1D_model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Conv1D_model, self).__init__()\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64 * 349, 512)  # Update the input dimension based on your GSR data length\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_physio_gsr_only(physio_model, val_dataloader, criterion, device):\n",
    "    # Validation phase\n",
    "    physio_model.eval() \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_vis_loss = 0.0\n",
    "    val_physio_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm(val_dataloader, total=len(val_dataloader), desc=f'Validation'):\n",
    "            val_inputs, val_labels = val_data\n",
    "\n",
    "            val_inputs = val_inputs.reshape(val_inputs.shape[0],1,val_inputs.shape[1])\n",
    "            \n",
    "            val_inputs = val_inputs.to(device, dtype=torch.float)\n",
    "            val_labels = val_labels.to(device)\n",
    "        \n",
    "\n",
    "\n",
    "            val_physio_outputs = physio_model(val_inputs)\n",
    "            # val_vis_outputs = vis_model(val_inputs)\n",
    "\n",
    "            val_physio_loss += criterion(val_physio_outputs, val_labels)\n",
    "            # val_vis_loss += criterion(val_vis_outputs, val_labels).item()\n",
    "\n",
    "            # val_both_outputs = val_physio_outputs + val_vis_outputs\n",
    "\n",
    "            _,val_predicted = torch.max(val_physio_outputs.data, 1)\n",
    "\n",
    "            # _, val_both_predicted = torch.max(val_both_outputs.data, 1)\n",
    "            \n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = ((val_physio_loss)) / len(val_dataloader)\n",
    "    print(f'Validation accuracy: {val_accuracy}%')\n",
    "    print(f'Validation loss: {avg_val_loss}')\n",
    "    return val_accuracy, avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m     weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_best_gsr_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    126\u001b[0m     weight_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_name, weight_name)\n\u001b[0;32m--> 127\u001b[0m     best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_annotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_annotation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     kfold_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m(best_accuracy, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[28], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_annotation, test_annotation, weight_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m train_annotation_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(five_fold_annotations_path, train_annotation)\n\u001b[1;32m     43\u001b[0m val_annotation_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(five_fold_annotations_path, test_annotation)\n\u001b[0;32m---> 45\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGSRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_annotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiosignals_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m GSRDataset(val_annotation_file, biosignals_path)\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mGSRDataset.__init__\u001b[0;34m(self, annotationfile_path, biosignals_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiosignals_path \u001b[38;5;241m=\u001b[39m biosignals_path\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotationfile_path \u001b[38;5;241m=\u001b[39m annotationfile_path\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_annotationfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m, in \u001b[0;36mGSRDataset._parse_annotationfile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotationfile_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m---> 31\u001b[0m         path, label \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(label)})\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_physio_gsr_only(physio_model, val_dataloader, criterion, device):\n",
    "    physio_model.eval() \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm(val_dataloader, total=len(val_dataloader), desc=f'Validation'):\n",
    "            val_inputs, val_labels = val_data\n",
    "            val_inputs = val_inputs.unsqueeze(1).to(device, dtype=torch.float)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = physio_model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f'Validation accuracy: {val_accuracy}%')\n",
    "    print(f'Validation loss: {avg_val_loss}')\n",
    "    return val_accuracy, avg_val_loss\n",
    "\n",
    "def train(train_annotation, test_annotation, weight_path):\n",
    "    batch_size = 1024\n",
    "    num_epochs = 200\n",
    "    lr = 0.0001\n",
    "    num_classes = 2\n",
    "    check_every = 1\n",
    "    best_val_acc = 0\n",
    "\n",
    "    biosignals_path = '/projets2/AS84330/Datasets/Biovid/PartA/physio/physio_organised'\n",
    "    five_fold_annotations_path = '/projets2/AS84330/Datasets/Biovid/PartA/5folds_annotations2/'\n",
    "    train_annotation_file = os.path.join(five_fold_annotations_path, train_annotation)\n",
    "    val_annotation_file = os.path.join(five_fold_annotations_path, test_annotation)\n",
    "\n",
    "    train_dataset = GSRDataset(train_annotation_file, biosignals_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GSRDataset(val_annotation_file, biosignals_path)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    physio_model = Conv1D_model(num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    physio_optimizer = optim.SGD(physio_model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "        physio_model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for physio_batch, labels in train_dataloader:\n",
    "            physio_optimizer.zero_grad()\n",
    "            physio_batch = physio_batch.unsqueeze(1).to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = physio_model(physio_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            physio_optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Accuracy after epoch {epoch + 1}: {100 * correct / total}%\")\n",
    "\n",
    "        if epoch % check_every == 0:\n",
    "            val_acc, val_loss = validate_physio_gsr_only(physio_model, val_dataloader, criterion, device)\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                model_save_path = f'{weight_path}{round(best_val_acc, 2)}.pth'\n",
    "                torch.save(physio_model.state_dict(), model_save_path)\n",
    "                print('Best model saved at epoch: ', epoch + 1)\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Training accuracy: {train_accuracy}%')\n",
    "    print(f'Training loss: {avg_train_loss}')\n",
    "    print(\"Best model saved at epoch: \", best_epoch)\n",
    "    print(\"Best validation accuracy: \", best_val_acc)\n",
    "    \n",
    "    return best_val_acc\n",
    "\n",
    "def test(test_annotation, test_weights):\n",
    "    batch_size = 1024\n",
    "    num_classes = 2\n",
    "    biosignals_path = '/projets2/AS84330/Datasets/Biovid/PartA/physio/physio_organised'\n",
    "    val_annotation_file = os.path.join(biosignals_path, '../../5folds_annotations', test_annotation)\n",
    "\n",
    "    val_dataset = GSRDataset(val_annotation_file, biosignals_path)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    physio_model = Conv1D_model(num_classes).to(device)\n",
    "    physio_model.load_state_dict(torch.load(test_weights))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    val_acc, _ = validate_physio_gsr_only(physio_model, val_dataloader, criterion, device)\n",
    "    \n",
    "    return val_acc\n",
    "\n",
    "##### Train + Evaluate #####\n",
    "if __name__ == '__main__':\n",
    "    dir_name = '/home/ens/AU59350/LIVIA/resultsCNN/'\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    kfold_accuracy = []\n",
    "    for i in range(1, 6):\n",
    "        train_annotation = f'train_fold{i}.txt'\n",
    "        test_annotation = f'test_fold{i}.txt'\n",
    "        weight_name = f'model_best_gsr_fold{i}_'\n",
    "        weight_path = os.path.join(dir_name, weight_name)\n",
    "        best_accuracy = train(train_annotation, test_annotation, weight_path)\n",
    "        kfold_accuracy.append(round(best_accuracy, 1))\n",
    "\n",
    "    with open(os.path.join(dir_name, 'accuracy.txt'), 'w') as f:\n",
    "        for acc in kfold_accuracy:\n",
    "            f.write(f'{acc}\\n')\n",
    "        f.write(f'Mean: {statistics.mean(kfold_accuracy)}\\n')\n",
    "\n",
    "##### Test#####\n",
    "# if __name__ == '__main__':\n",
    "#     kfold_accuracy = []\n",
    "#     for i in range(1, 6):\n",
    "#         test_annotation = f'test_fold{i}.txt'\n",
    "#         weight_name = f'model_best_gsr_fold{i}.pth'\n",
    "#         best_accuracy = test(test_annotation, weight_name)\n",
    "#         kfold_accuracy.append(round(best_accuracy, 1))\n",
    "\n",
    "#     print('Accuracy on fold 1:', kfold_accuracy[0])\n",
    "#     print('Accuracy on fold 2:', kfold_accuracy[1])\n",
    "#     print('Accuracy on fold 3:', kfold_accuracy[2])\n",
    "#     print('Accuracy on fold 4:', kfold_accuracy[3])\n",
    "#     print('Accuracy on fold 5:', kfold_accuracy[4])\n",
    "#     print('Mean:', statistics.mean(kfold_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_annotation,test_annotation,weight_path):\n",
    "    batch_size = 1024\n",
    "    num_epochs = 200\n",
    "    lr = 0.0001\n",
    "    num_classes = 2\n",
    "    check_every = 1\n",
    "    best_val_acc = 0\n",
    "\n",
    "    biosignals_path = '/projets2/AS84330/Datasets/Biovid/PartA/physio/physio_organised'\n",
    "    five_fold_annotations_path = '/projets2/AS84330/Datasets/Biovid/PartA/5folds_annotations2/'\n",
    "    train_annotation_file = os.path.join(five_fold_annotations_path, train_annotation)\n",
    "    val_annotation_file = os.path.join(five_fold_annotations_path, test_annotation)\n",
    "\n",
    "\n",
    "    train_dataset = GSRDataset(train_annotation_file, biosignals_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GSRDataset(val_annotation_file, biosignals_path)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    physio_model = Conv1D_model(num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    physio_optimizer = optim.SGD(physio_model.parameters(), lr=lr, momentum=0.9)\n",
    "    #scheduler = ReduceLROnPlateau(physio_optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "        physio_model.train()\n",
    "        \n",
    "        \n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i,(physio_batch, labels) in enumerate(train_dataloader):\n",
    "            physio_optimizer.zero_grad()\n",
    "            physio_batch = physio_batch.reshape(physio_batch.shape[0],1,physio_batch.shape[1])\n",
    "            physio_batch = physio_batch.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            physio_outputs = physio_model(physio_batch)\n",
    "            \n",
    "            physio_loss = criterion(physio_outputs, labels)\n",
    "            \n",
    "            physio_loss.backward()\n",
    "            physio_optimizer.step()\n",
    "            # print(physio_loss.data)\n",
    "            \n",
    "            running_loss += physio_loss.item()\n",
    "            \n",
    "            _, physio_predicted = torch.max(physio_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # print('output: ', physio_outputs)\n",
    "            # print('predicted: ', physio_predicted)\n",
    "            # print('labels: ', labels)\n",
    "            # print('**************************')\n",
    "            correct += (physio_predicted == labels).sum().item()\n",
    "            #print(physio_loss.item())\n",
    "\n",
    "        print(f\"Accuracy after epoch {epoch + 1}: {100 * correct / total}%\")\n",
    "\n",
    "        if epoch % check_every == 0:\n",
    "                val_acc, val_loss = validate_physio_gsr_only(physio_model, val_dataloader, criterion, device)\n",
    "                # scheduler.step(val_loss)\n",
    "                # print( \"Validation accuracy: \", val_acc)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    remove_previous_files(weight_path)\n",
    "                    model_save_path = f'{weight_path}{round(best_val_acc,2)}.pth'\n",
    "                    torch.save(physio_model.state_dict(), model_save_path)\n",
    "                    print('Best model saved at epoch: ', epoch+1)\n",
    "                    best_epoch = epoch+1\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Training accuracy: {train_accuracy}%')\n",
    "    print(f'Training loss: {avg_train_loss}')\n",
    "\n",
    "    print(\"Best model saved at epoch: \", best_epoch)\n",
    "    print(\"Best validation accuracy: \", best_val_acc)\n",
    "    \n",
    "    return best_val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClaraEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
