{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge Distillation \n",
    "===============================\n",
    "\n",
    "**Author**: [Clara Martinez](https://github.com/moonblume/LIVIA.git)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a technique that enables knowledge transfer\n",
    "from large, computationally expensive models to smaller ones without\n",
    "losing validity. This allows for deployment on less powerful hardware,\n",
    "making evaluation faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librairies\n",
    "================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from typing import List, Union, Tuple, Any\n",
    "import statistics\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset\n",
    "================\n",
    "\n",
    "Fisrt, I focus on the physiological signals of the Biovid dataset. In one sample, we have access to 6 classes associated with 0 to 4 pain levels :  \n",
    "\n",
    "Time: This could be the timestamp or time index when the signal was recorded.\n",
    "\n",
    "GSR (Galvanic Skin Response): A measure of the electrical conductance of the skin, which varies with the moisture level of the skin. It's often associated with emotional arousal.\n",
    "\n",
    "ECG (Electrocardiogram): A recording of the electrical activity of the heart over time. It typically consists of waves representing the depolarization and repolarization of the heart muscle during each heartbeat.\n",
    "\n",
    "EMG (Electromyography) - Trapezius: Measures the electrical activity produced by skeletal muscles. The trapezius muscle is a large superficial muscle that extends longitudinally from the occipital bone to the lower thoracic vertebrae and laterally to the spine of the scapula.\n",
    "\n",
    "EMG - Corrugator: Electromyography signal from the corrugator supercilii muscle, which is a small facial muscle involved in frowning and expressing negative emotions.\n",
    "\n",
    "EMG - Zygomaticus: Electromyography signal from the zygomaticus major muscle, which is involved in smiling and expressing positive emotions.  \n",
    "    \n",
    "    \n",
    "Our objective is to predict the pain level of input signals. One signal corresponds to one csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>072414_m_23-PA2-034_bio.csv</td>\n",
       "      <td>[6.966839, 6.966161, 6.966, 6.966839, 6.966161...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081609_w_40-PA2-028_bio.csv</td>\n",
       "      <td>[0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081714_m_36-PA2-065_bio.csv</td>\n",
       "      <td>[6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102514_w_40-PA2-046_bio.csv</td>\n",
       "      <td>[1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120514_w_56-PA2-019_bio.csv</td>\n",
       "      <td>[2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CSV name  \\\n",
       "0  072414_m_23-PA2-034_bio.csv   \n",
       "1  081609_w_40-PA2-028_bio.csv   \n",
       "2  081714_m_36-PA2-065_bio.csv   \n",
       "3  102514_w_40-PA2-046_bio.csv   \n",
       "4  120514_w_56-PA2-019_bio.csv   \n",
       "\n",
       "                                         GSR signals  Pain level  \n",
       "0  [6.966839, 6.966161, 6.966, 6.966839, 6.966161...           2  \n",
       "1  [0.872, 0.872, 0.872, 0.872, 0.872, 0.872, 0.8...           2  \n",
       "2  [6.089862, 6.091, 6.091432, 6.092, 6.092432, 6...           2  \n",
       "3  [1.462, 1.462, 1.462, 1.462, 1.462, 1.462, 1.4...           2  \n",
       "4  [2.226, 2.226, 2.226, 2.226, 2.226, 2.226, 2.2...           2  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory containing the CSV files\n",
    "biosignals_path = '/home/ens/AU59350/LIVIA/physio/physio_organised/'\n",
    "\n",
    "# Initialize an empty list to store data for DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate over each pain level directory\n",
    "for pain_level in os.listdir(biosignals_path):\n",
    "    pain_level_dir = os.path.join(biosignals_path, pain_level)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(pain_level_dir):\n",
    "        # Iterate over each CSV file in the pain level directory\n",
    "        for csv_file in os.listdir(pain_level_dir):\n",
    "            # Check if it's a CSV file\n",
    "            if csv_file.endswith('.csv'):\n",
    "                csv_path = os.path.join(pain_level_dir, csv_file)\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(csv_path, sep='\\t')\n",
    "                # Extract GSR values\n",
    "                gsr_signal = df['gsr'].values\n",
    "                # Append the CSV name, GSR signals, and Pain level to the data list\n",
    "                data.append({'CSV name': csv_file, 'GSR signals': gsr_signal, 'Pain level': int(pain_level)})\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps for GSR DataFrame include tasks such as handling missing values, smoothing the signal to reduce noise in the GSR signal 9(Savitzky-Golay filtering), removing outliers (z-score), and normalizing the data between a specified range, such as [0, 1] or [-1, 1] helping comparison across different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_285565/1813134786.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  gsr_signal_normalized = (gsr_signal_smooth_no_outliers - gsr_signal_smooth_no_outliers.min()) / \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV name</th>\n",
       "      <th>GSR signals</th>\n",
       "      <th>Pain level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>072414_m_23-PA2-034_bio.csv</td>\n",
       "      <td>[0.9899913791193845, 0.9877131683489699, 0.986...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>081609_w_40-PA2-028_bio.csv</td>\n",
       "      <td>[0.9678938384198197, 0.9678938384197773, 0.967...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>081714_m_36-PA2-065_bio.csv</td>\n",
       "      <td>[0.9020912464663925, 0.9035627741584942, 0.904...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102514_w_40-PA2-046_bio.csv</td>\n",
       "      <td>[0.9675465896733677, 0.9675465896732949, 0.967...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120514_w_56-PA2-019_bio.csv</td>\n",
       "      <td>[0.9986433711307843, 0.9986433711307986, 0.998...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      CSV name  \\\n",
       "0  072414_m_23-PA2-034_bio.csv   \n",
       "1  081609_w_40-PA2-028_bio.csv   \n",
       "2  081714_m_36-PA2-065_bio.csv   \n",
       "3  102514_w_40-PA2-046_bio.csv   \n",
       "4  120514_w_56-PA2-019_bio.csv   \n",
       "\n",
       "                                         GSR signals  Pain level  \n",
       "0  [0.9899913791193845, 0.9877131683489699, 0.986...           2  \n",
       "1  [0.9678938384198197, 0.9678938384197773, 0.967...           2  \n",
       "2  [0.9020912464663925, 0.9035627741584942, 0.904...           2  \n",
       "3  [0.9675465896733677, 0.9675465896732949, 0.967...           2  \n",
       "4  [0.9986433711307843, 0.9986433711307986, 0.998...           2  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to preprocess GSR signals\n",
    "def preprocess_gsr_signal(gsr_signal):\n",
    "    # Handle missing values (if any)\n",
    "    gsr_signal = np.array(gsr_signal)  # Convert to NumPy array\n",
    "    gsr_signal = gsr_signal[~np.isnan(gsr_signal)]  # Remove NaN values\n",
    "    \n",
    "    # Check if the length of the signal is sufficient for smoothing\n",
    "    if len(gsr_signal) < 5:\n",
    "        # If the signal is too short, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    try:\n",
    "        # Smoothing using Savitzky-Golay filter\n",
    "        gsr_signal_smooth = savgol_filter(gsr_signal, window_length=5, polyorder=2)\n",
    "    except ValueError:\n",
    "        # If an error occurs during smoothing, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    # Removing outliers based on Z-scores\n",
    "    z_scores = (gsr_signal_smooth - gsr_signal_smooth.mean()) / gsr_signal_smooth.std()\n",
    "    gsr_signal_smooth_no_outliers = gsr_signal_smooth[(z_scores < 3)]\n",
    "    \n",
    "    # Normalization\n",
    "    if len(gsr_signal_smooth_no_outliers) > 0:\n",
    "        gsr_signal_normalized = (gsr_signal_smooth_no_outliers - gsr_signal_smooth_no_outliers.min()) / \\\n",
    "                                 (gsr_signal_smooth_no_outliers.max() - gsr_signal_smooth_no_outliers.min())\n",
    "    else:\n",
    "        # If there are no valid values after removing outliers, return the original signal\n",
    "        return gsr_signal\n",
    "    \n",
    "    return gsr_signal_normalized\n",
    "\n",
    "# Apply preprocessing to each row in the DataFrame\n",
    "df['GSR signals'] = df['GSR signals'].apply(preprocess_gsr_signal)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([6960, 1, 2816])\n",
      "X_test_tensor shape: torch.Size([1740, 1, 2816])\n",
      "y_train_tensor shape: torch.Size([6960])\n",
      "y_test_tensor shape: torch.Size([1740])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the data\n",
    "max_length = max(len(signal) for signal in df['GSR signals'])  # Find the maximum length of GSR signals\n",
    "\n",
    "# Pad or truncate the GSR signals to the maximum length\n",
    "gsr_signals = np.array([np.pad(signal, (0, max_length - len(signal))) if len(signal) < max_length else signal[:max_length] for signal in df['GSR signals']])\n",
    "\n",
    "pain_levels = df['Pain level'].values\n",
    "\n",
    "# Step 2: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(gsr_signals, pain_levels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)  \n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Check the shape of tensors\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_tensor: The training data tensor with a shape of [6960, 1, 2816], indicating that there are 6960 samples, each with 1 channel (for the GSR signal), and each signal has been padded or truncated to a length of 2816.  \n",
    "X_test_tensor: The test data tensor with a shape of [1740, 1, 2816], indicating that there are 1740 samples in the test set, each with 1 channel, and the signals have the same length as the training data.  \n",
    "y_train_tensor: The training labels tensor with a shape of [6960], containing the corresponding pain levels for the training samples.  \n",
    "y_test_tensor: The test labels tensor with a shape of [1740], containing the corresponding pain levels for the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([816, 1, 2816])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dimensionality of the tensor\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension (29) represents the batch size, indicating that there are 29 samples in the batch.  \n",
    "The second dimension (1) represents the number of channels. In this case, there is only one channel.  \n",
    "The third dimension (1) represents the length of the input data for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data has a shape of (batch_size, channels, sequence_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1D_model(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(2,))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=22336, out_features=512, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch 1/100, Loss: 1.6100\n",
      "Epoch 2/100, Loss: 1.6099\n",
      "Epoch 3/100, Loss: 1.6098\n",
      "Epoch 4/100, Loss: 1.6097\n",
      "Epoch 5/100, Loss: 1.6095\n",
      "Epoch 6/100, Loss: 1.6094\n",
      "Epoch 7/100, Loss: 1.6093\n",
      "Epoch 8/100, Loss: 1.6092\n",
      "Epoch 9/100, Loss: 1.6091\n",
      "Epoch 10/100, Loss: 1.6090\n",
      "Epoch 11/100, Loss: 1.6089\n",
      "Epoch 12/100, Loss: 1.6087\n",
      "Epoch 13/100, Loss: 1.6086\n",
      "Epoch 14/100, Loss: 1.6085\n",
      "Epoch 15/100, Loss: 1.6084\n",
      "Epoch 16/100, Loss: 1.6083\n",
      "Epoch 17/100, Loss: 1.6081\n",
      "Epoch 18/100, Loss: 1.6080\n",
      "Epoch 19/100, Loss: 1.6079\n",
      "Epoch 20/100, Loss: 1.6078\n",
      "Epoch 21/100, Loss: 1.6076\n",
      "Epoch 22/100, Loss: 1.6075\n",
      "Epoch 23/100, Loss: 1.6075\n",
      "Epoch 24/100, Loss: 1.6073\n",
      "Epoch 25/100, Loss: 1.6072\n",
      "Epoch 26/100, Loss: 1.6071\n",
      "Epoch 27/100, Loss: 1.6070\n",
      "Epoch 28/100, Loss: 1.6069\n",
      "Epoch 29/100, Loss: 1.6067\n",
      "Epoch 30/100, Loss: 1.6066\n",
      "Epoch 31/100, Loss: 1.6065\n",
      "Epoch 32/100, Loss: 1.6064\n",
      "Epoch 33/100, Loss: 1.6063\n",
      "Epoch 34/100, Loss: 1.6061\n",
      "Epoch 35/100, Loss: 1.6060\n",
      "Epoch 36/100, Loss: 1.6059\n",
      "Epoch 37/100, Loss: 1.6058\n",
      "Epoch 38/100, Loss: 1.6057\n",
      "Epoch 39/100, Loss: 1.6055\n",
      "Epoch 40/100, Loss: 1.6054\n",
      "Epoch 41/100, Loss: 1.6053\n",
      "Epoch 42/100, Loss: 1.6052\n",
      "Epoch 43/100, Loss: 1.6051\n",
      "Epoch 44/100, Loss: 1.6050\n",
      "Epoch 45/100, Loss: 1.6048\n",
      "Epoch 46/100, Loss: 1.6047\n",
      "Epoch 47/100, Loss: 1.6046\n",
      "Epoch 48/100, Loss: 1.6045\n",
      "Epoch 49/100, Loss: 1.6044\n",
      "Epoch 50/100, Loss: 1.6042\n",
      "Epoch 51/100, Loss: 1.6042\n",
      "Epoch 52/100, Loss: 1.6040\n",
      "Epoch 53/100, Loss: 1.6039\n",
      "Epoch 54/100, Loss: 1.6038\n",
      "Epoch 55/100, Loss: 1.6036\n",
      "Epoch 56/100, Loss: 1.6035\n",
      "Epoch 57/100, Loss: 1.6034\n",
      "Epoch 58/100, Loss: 1.6033\n",
      "Epoch 59/100, Loss: 1.6032\n",
      "Epoch 60/100, Loss: 1.6030\n",
      "Epoch 61/100, Loss: 1.6029\n",
      "Epoch 62/100, Loss: 1.6029\n",
      "Epoch 63/100, Loss: 1.6027\n",
      "Epoch 64/100, Loss: 1.6026\n",
      "Epoch 65/100, Loss: 1.6025\n",
      "Epoch 66/100, Loss: 1.6024\n",
      "Epoch 67/100, Loss: 1.6022\n",
      "Epoch 68/100, Loss: 1.6021\n",
      "Epoch 69/100, Loss: 1.6020\n",
      "Epoch 70/100, Loss: 1.6018\n",
      "Epoch 71/100, Loss: 1.6017\n",
      "Epoch 72/100, Loss: 1.6016\n",
      "Epoch 73/100, Loss: 1.6015\n",
      "Epoch 74/100, Loss: 1.6014\n",
      "Epoch 75/100, Loss: 1.6013\n",
      "Epoch 76/100, Loss: 1.6012\n",
      "Epoch 77/100, Loss: 1.6010\n",
      "Epoch 78/100, Loss: 1.6009\n",
      "Epoch 79/100, Loss: 1.6008\n",
      "Epoch 80/100, Loss: 1.6007\n",
      "Epoch 81/100, Loss: 1.6006\n",
      "Epoch 82/100, Loss: 1.6005\n",
      "Epoch 83/100, Loss: 1.6003\n",
      "Epoch 84/100, Loss: 1.6002\n",
      "Epoch 85/100, Loss: 1.6002\n",
      "Epoch 86/100, Loss: 1.6000\n",
      "Epoch 87/100, Loss: 1.5999\n",
      "Epoch 88/100, Loss: 1.5997\n",
      "Epoch 89/100, Loss: 1.5996\n",
      "Epoch 90/100, Loss: 1.5995\n",
      "Epoch 91/100, Loss: 1.5994\n",
      "Epoch 92/100, Loss: 1.5993\n",
      "Epoch 93/100, Loss: 1.5992\n",
      "Epoch 94/100, Loss: 1.5990\n",
      "Epoch 95/100, Loss: 1.5989\n",
      "Epoch 96/100, Loss: 1.5988\n",
      "Epoch 97/100, Loss: 1.5987\n",
      "Epoch 98/100, Loss: 1.5986\n",
      "Epoch 99/100, Loss: 1.5984\n",
      "Epoch 100/100, Loss: 1.5983\n",
      "Test Accuracy: 0.2167\n"
     ]
    }
   ],
   "source": [
    "# Define the Conv1D model\n",
    "class Conv1D_model(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Conv1D_model, self).__init__()\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(22336, 512)  \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = Conv1D_model(num_classes=5)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Define learning parameters\n",
    "lr = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 1024\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Define training and testing tensors (replace with your actual data)\n",
    "X_train_tensor = torch.randn(6960, 1, 2816)\n",
    "y_train_tensor = torch.randint(0, 5, (6960,))\n",
    "X_test_tensor = torch.randn(1740, 1, 2816)\n",
    "y_test_tensor = torch.randint(0, 5, (1740,))\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    y_test_tensor_sampled = y_test_tensor[:len(predictions)]\n",
    "    accuracy = torch.mean((predictions == y_test_tensor_sampled).float()).item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/100, Loss: 1.6199, Accuracy: 0.2044\n",
      "Epoch 2/100, Loss: 1.6155, Accuracy: 0.2042\n",
      "Epoch 3/100, Loss: 1.6117, Accuracy: 0.2051\n",
      "Epoch 4/100, Loss: 1.6103, Accuracy: 0.2055\n",
      "Epoch 5/100, Loss: 1.6099, Accuracy: 0.2094\n",
      "Epoch 6/100, Loss: 1.6099, Accuracy: 0.2119\n",
      "Epoch 7/100, Loss: 1.6098, Accuracy: 0.2125\n",
      "Epoch 8/100, Loss: 1.6096, Accuracy: 0.2123\n",
      "Epoch 9/100, Loss: 1.6094, Accuracy: 0.2119\n",
      "Epoch 10/100, Loss: 1.6092, Accuracy: 0.2117\n",
      "Epoch 11/100, Loss: 1.6091, Accuracy: 0.2123\n",
      "Epoch 12/100, Loss: 1.6089, Accuracy: 0.2108\n",
      "Epoch 13/100, Loss: 1.6088, Accuracy: 0.2094\n",
      "Epoch 14/100, Loss: 1.6087, Accuracy: 0.2101\n",
      "Epoch 15/100, Loss: 1.6086, Accuracy: 0.2119\n",
      "Epoch 16/100, Loss: 1.6084, Accuracy: 0.2110\n",
      "Epoch 17/100, Loss: 1.6083, Accuracy: 0.2110\n",
      "Epoch 18/100, Loss: 1.6082, Accuracy: 0.2139\n",
      "Epoch 19/100, Loss: 1.6080, Accuracy: 0.2143\n",
      "Epoch 20/100, Loss: 1.6079, Accuracy: 0.2135\n",
      "Epoch 21/100, Loss: 1.6077, Accuracy: 0.2144\n",
      "Epoch 22/100, Loss: 1.6077, Accuracy: 0.2164\n",
      "Epoch 23/100, Loss: 1.6075, Accuracy: 0.2173\n",
      "Epoch 24/100, Loss: 1.6074, Accuracy: 0.2161\n",
      "Epoch 25/100, Loss: 1.6073, Accuracy: 0.2179\n",
      "Epoch 26/100, Loss: 1.6071, Accuracy: 0.2196\n",
      "Epoch 27/100, Loss: 1.6070, Accuracy: 0.2173\n",
      "Epoch 28/100, Loss: 1.6069, Accuracy: 0.2180\n",
      "Epoch 29/100, Loss: 1.6068, Accuracy: 0.2173\n",
      "Epoch 30/100, Loss: 1.6066, Accuracy: 0.2150\n",
      "Epoch 31/100, Loss: 1.6065, Accuracy: 0.2186\n",
      "Epoch 32/100, Loss: 1.6063, Accuracy: 0.2184\n",
      "Epoch 33/100, Loss: 1.6062, Accuracy: 0.2218\n",
      "Epoch 34/100, Loss: 1.6060, Accuracy: 0.2229\n",
      "Epoch 35/100, Loss: 1.6060, Accuracy: 0.2236\n",
      "Epoch 36/100, Loss: 1.6058, Accuracy: 0.2243\n",
      "Epoch 37/100, Loss: 1.6057, Accuracy: 0.2268\n",
      "Epoch 38/100, Loss: 1.6055, Accuracy: 0.2265\n",
      "Epoch 39/100, Loss: 1.6054, Accuracy: 0.2252\n",
      "Epoch 40/100, Loss: 1.6053, Accuracy: 0.2232\n",
      "Epoch 41/100, Loss: 1.6051, Accuracy: 0.2241\n",
      "Epoch 42/100, Loss: 1.6050, Accuracy: 0.2252\n",
      "Epoch 43/100, Loss: 1.6049, Accuracy: 0.2252\n",
      "Epoch 44/100, Loss: 1.6048, Accuracy: 0.2279\n",
      "Epoch 45/100, Loss: 1.6046, Accuracy: 0.2297\n",
      "Epoch 46/100, Loss: 1.6045, Accuracy: 0.2254\n",
      "Epoch 47/100, Loss: 1.6043, Accuracy: 0.2279\n",
      "Epoch 48/100, Loss: 1.6042, Accuracy: 0.2297\n",
      "Epoch 49/100, Loss: 1.6041, Accuracy: 0.2301\n",
      "Epoch 50/100, Loss: 1.6040, Accuracy: 0.2297\n",
      "Epoch 51/100, Loss: 1.6038, Accuracy: 0.2299\n",
      "Epoch 52/100, Loss: 1.6037, Accuracy: 0.2328\n",
      "Epoch 53/100, Loss: 1.6037, Accuracy: 0.2333\n",
      "Epoch 54/100, Loss: 1.6034, Accuracy: 0.2365\n",
      "Epoch 55/100, Loss: 1.6033, Accuracy: 0.2338\n",
      "Epoch 56/100, Loss: 1.6032, Accuracy: 0.2338\n",
      "Epoch 57/100, Loss: 1.6030, Accuracy: 0.2360\n",
      "Epoch 58/100, Loss: 1.6029, Accuracy: 0.2351\n",
      "Epoch 59/100, Loss: 1.6028, Accuracy: 0.2333\n",
      "Epoch 60/100, Loss: 1.6027, Accuracy: 0.2333\n",
      "Epoch 61/100, Loss: 1.6025, Accuracy: 0.2374\n",
      "Epoch 62/100, Loss: 1.6024, Accuracy: 0.2360\n",
      "Epoch 63/100, Loss: 1.6023, Accuracy: 0.2356\n",
      "Epoch 64/100, Loss: 1.6021, Accuracy: 0.2390\n",
      "Epoch 65/100, Loss: 1.6020, Accuracy: 0.2426\n",
      "Epoch 66/100, Loss: 1.6019, Accuracy: 0.2439\n",
      "Epoch 67/100, Loss: 1.6018, Accuracy: 0.2439\n",
      "Epoch 68/100, Loss: 1.6016, Accuracy: 0.2401\n",
      "Epoch 69/100, Loss: 1.6015, Accuracy: 0.2417\n",
      "Epoch 70/100, Loss: 1.6013, Accuracy: 0.2417\n",
      "Epoch 71/100, Loss: 1.6012, Accuracy: 0.2428\n",
      "Epoch 72/100, Loss: 1.6011, Accuracy: 0.2443\n",
      "Epoch 73/100, Loss: 1.6010, Accuracy: 0.2493\n",
      "Epoch 74/100, Loss: 1.6008, Accuracy: 0.2509\n",
      "Epoch 75/100, Loss: 1.6007, Accuracy: 0.2464\n",
      "Epoch 76/100, Loss: 1.6006, Accuracy: 0.2439\n",
      "Epoch 77/100, Loss: 1.6004, Accuracy: 0.2394\n",
      "Epoch 78/100, Loss: 1.6003, Accuracy: 0.2407\n",
      "Epoch 79/100, Loss: 1.6001, Accuracy: 0.2448\n",
      "Epoch 80/100, Loss: 1.6001, Accuracy: 0.2473\n",
      "Epoch 81/100, Loss: 1.5999, Accuracy: 0.2473\n",
      "Epoch 82/100, Loss: 1.5998, Accuracy: 0.2460\n",
      "Epoch 83/100, Loss: 1.5997, Accuracy: 0.2459\n",
      "Epoch 84/100, Loss: 1.5995, Accuracy: 0.2487\n",
      "Epoch 85/100, Loss: 1.5994, Accuracy: 0.2525\n",
      "Epoch 86/100, Loss: 1.5992, Accuracy: 0.2540\n",
      "Epoch 87/100, Loss: 1.5991, Accuracy: 0.2550\n",
      "Epoch 88/100, Loss: 1.5990, Accuracy: 0.2516\n",
      "Epoch 89/100, Loss: 1.5988, Accuracy: 0.2505\n",
      "Epoch 90/100, Loss: 1.5987, Accuracy: 0.2520\n",
      "Epoch 91/100, Loss: 1.5986, Accuracy: 0.2552\n",
      "Epoch 92/100, Loss: 1.5985, Accuracy: 0.2514\n",
      "Epoch 93/100, Loss: 1.5983, Accuracy: 0.2534\n",
      "Epoch 94/100, Loss: 1.5982, Accuracy: 0.2556\n",
      "Epoch 95/100, Loss: 1.5981, Accuracy: 0.2617\n",
      "Epoch 96/100, Loss: 1.5979, Accuracy: 0.2583\n",
      "Epoch 97/100, Loss: 1.5978, Accuracy: 0.2583\n",
      "Epoch 98/100, Loss: 1.5977, Accuracy: 0.2597\n",
      "Epoch 99/100, Loss: 1.5975, Accuracy: 0.2631\n",
      "Epoch 100/100, Loss: 1.5974, Accuracy: 0.2613\n",
      "Validation Accuracy: 0.1897\n",
      "Fold 2/5\n",
      "Epoch 1/100, Loss: 1.6104, Accuracy: 0.2067\n",
      "Epoch 2/100, Loss: 1.6094, Accuracy: 0.2119\n",
      "Epoch 3/100, Loss: 1.6087, Accuracy: 0.2152\n",
      "Epoch 4/100, Loss: 1.6084, Accuracy: 0.2157\n",
      "Epoch 5/100, Loss: 1.6084, Accuracy: 0.2162\n",
      "Epoch 6/100, Loss: 1.6083, Accuracy: 0.2173\n",
      "Epoch 7/100, Loss: 1.6081, Accuracy: 0.2173\n",
      "Epoch 8/100, Loss: 1.6079, Accuracy: 0.2198\n",
      "Epoch 9/100, Loss: 1.6078, Accuracy: 0.2209\n",
      "Epoch 10/100, Loss: 1.6077, Accuracy: 0.2193\n",
      "Epoch 11/100, Loss: 1.6076, Accuracy: 0.2198\n",
      "Epoch 12/100, Loss: 1.6074, Accuracy: 0.2207\n",
      "Epoch 13/100, Loss: 1.6072, Accuracy: 0.2193\n",
      "Epoch 14/100, Loss: 1.6071, Accuracy: 0.2179\n",
      "Epoch 15/100, Loss: 1.6070, Accuracy: 0.2207\n",
      "Epoch 16/100, Loss: 1.6068, Accuracy: 0.2258\n",
      "Epoch 17/100, Loss: 1.6067, Accuracy: 0.2268\n",
      "Epoch 18/100, Loss: 1.6066, Accuracy: 0.2268\n",
      "Epoch 19/100, Loss: 1.6064, Accuracy: 0.2297\n",
      "Epoch 20/100, Loss: 1.6063, Accuracy: 0.2277\n",
      "Epoch 21/100, Loss: 1.6061, Accuracy: 0.2279\n",
      "Epoch 22/100, Loss: 1.6060, Accuracy: 0.2288\n",
      "Epoch 23/100, Loss: 1.6058, Accuracy: 0.2320\n",
      "Epoch 24/100, Loss: 1.6057, Accuracy: 0.2270\n",
      "Epoch 25/100, Loss: 1.6057, Accuracy: 0.2256\n",
      "Epoch 26/100, Loss: 1.6055, Accuracy: 0.2322\n",
      "Epoch 27/100, Loss: 1.6053, Accuracy: 0.2322\n",
      "Epoch 28/100, Loss: 1.6051, Accuracy: 0.2333\n",
      "Epoch 29/100, Loss: 1.6050, Accuracy: 0.2365\n",
      "Epoch 30/100, Loss: 1.6049, Accuracy: 0.2380\n",
      "Epoch 31/100, Loss: 1.6047, Accuracy: 0.2376\n",
      "Epoch 32/100, Loss: 1.6046, Accuracy: 0.2356\n",
      "Epoch 33/100, Loss: 1.6045, Accuracy: 0.2365\n",
      "Epoch 34/100, Loss: 1.6043, Accuracy: 0.2378\n",
      "Epoch 35/100, Loss: 1.6042, Accuracy: 0.2396\n",
      "Epoch 36/100, Loss: 1.6040, Accuracy: 0.2372\n",
      "Epoch 37/100, Loss: 1.6039, Accuracy: 0.2360\n",
      "Epoch 38/100, Loss: 1.6038, Accuracy: 0.2408\n",
      "Epoch 39/100, Loss: 1.6036, Accuracy: 0.2437\n",
      "Epoch 40/100, Loss: 1.6035, Accuracy: 0.2432\n",
      "Epoch 41/100, Loss: 1.6034, Accuracy: 0.2435\n",
      "Epoch 42/100, Loss: 1.6032, Accuracy: 0.2452\n",
      "Epoch 43/100, Loss: 1.6031, Accuracy: 0.2484\n",
      "Epoch 44/100, Loss: 1.6029, Accuracy: 0.2489\n",
      "Epoch 45/100, Loss: 1.6029, Accuracy: 0.2480\n",
      "Epoch 46/100, Loss: 1.6027, Accuracy: 0.2482\n",
      "Epoch 47/100, Loss: 1.6025, Accuracy: 0.2486\n",
      "Epoch 48/100, Loss: 1.6024, Accuracy: 0.2475\n",
      "Epoch 49/100, Loss: 1.6023, Accuracy: 0.2477\n",
      "Epoch 50/100, Loss: 1.6021, Accuracy: 0.2475\n",
      "Epoch 51/100, Loss: 1.6020, Accuracy: 0.2538\n",
      "Epoch 52/100, Loss: 1.6018, Accuracy: 0.2550\n",
      "Epoch 53/100, Loss: 1.6017, Accuracy: 0.2513\n",
      "Epoch 54/100, Loss: 1.6016, Accuracy: 0.2531\n",
      "Epoch 55/100, Loss: 1.6014, Accuracy: 0.2531\n",
      "Epoch 56/100, Loss: 1.6013, Accuracy: 0.2525\n",
      "Epoch 57/100, Loss: 1.6011, Accuracy: 0.2538\n",
      "Epoch 58/100, Loss: 1.6010, Accuracy: 0.2574\n",
      "Epoch 59/100, Loss: 1.6009, Accuracy: 0.2602\n",
      "Epoch 60/100, Loss: 1.6008, Accuracy: 0.2556\n",
      "Epoch 61/100, Loss: 1.6006, Accuracy: 0.2570\n",
      "Epoch 62/100, Loss: 1.6004, Accuracy: 0.2608\n",
      "Epoch 63/100, Loss: 1.6003, Accuracy: 0.2608\n",
      "Epoch 64/100, Loss: 1.6001, Accuracy: 0.2601\n",
      "Epoch 65/100, Loss: 1.6000, Accuracy: 0.2624\n",
      "Epoch 66/100, Loss: 1.5999, Accuracy: 0.2660\n",
      "Epoch 67/100, Loss: 1.5998, Accuracy: 0.2645\n",
      "Epoch 68/100, Loss: 1.5997, Accuracy: 0.2642\n",
      "Epoch 69/100, Loss: 1.5995, Accuracy: 0.2636\n",
      "Epoch 70/100, Loss: 1.5993, Accuracy: 0.2631\n",
      "Epoch 71/100, Loss: 1.5992, Accuracy: 0.2610\n",
      "Epoch 72/100, Loss: 1.5990, Accuracy: 0.2601\n",
      "Epoch 73/100, Loss: 1.5989, Accuracy: 0.2662\n",
      "Epoch 74/100, Loss: 1.5988, Accuracy: 0.2698\n",
      "Epoch 75/100, Loss: 1.5986, Accuracy: 0.2753\n",
      "Epoch 76/100, Loss: 1.5985, Accuracy: 0.2724\n",
      "Epoch 77/100, Loss: 1.5984, Accuracy: 0.2672\n",
      "Epoch 78/100, Loss: 1.5982, Accuracy: 0.2671\n",
      "Epoch 79/100, Loss: 1.5981, Accuracy: 0.2667\n",
      "Epoch 80/100, Loss: 1.5979, Accuracy: 0.2712\n",
      "Epoch 81/100, Loss: 1.5978, Accuracy: 0.2721\n",
      "Epoch 82/100, Loss: 1.5977, Accuracy: 0.2751\n",
      "Epoch 83/100, Loss: 1.5975, Accuracy: 0.2768\n",
      "Epoch 84/100, Loss: 1.5974, Accuracy: 0.2750\n",
      "Epoch 85/100, Loss: 1.5972, Accuracy: 0.2717\n",
      "Epoch 86/100, Loss: 1.5971, Accuracy: 0.2786\n",
      "Epoch 87/100, Loss: 1.5970, Accuracy: 0.2839\n",
      "Epoch 88/100, Loss: 1.5968, Accuracy: 0.2838\n",
      "Epoch 89/100, Loss: 1.5966, Accuracy: 0.2825\n",
      "Epoch 90/100, Loss: 1.5965, Accuracy: 0.2818\n",
      "Epoch 91/100, Loss: 1.5964, Accuracy: 0.2802\n",
      "Epoch 92/100, Loss: 1.5963, Accuracy: 0.2768\n",
      "Epoch 93/100, Loss: 1.5961, Accuracy: 0.2759\n",
      "Epoch 94/100, Loss: 1.5960, Accuracy: 0.2755\n",
      "Epoch 95/100, Loss: 1.5958, Accuracy: 0.2791\n",
      "Epoch 96/100, Loss: 1.5957, Accuracy: 0.2845\n",
      "Epoch 97/100, Loss: 1.5955, Accuracy: 0.2875\n",
      "Epoch 98/100, Loss: 1.5954, Accuracy: 0.2872\n",
      "Epoch 99/100, Loss: 1.5952, Accuracy: 0.2892\n",
      "Epoch 100/100, Loss: 1.5951, Accuracy: 0.2859\n",
      "Validation Accuracy: 0.2126\n",
      "Fold 3/5\n",
      "Epoch 1/100, Loss: 1.6140, Accuracy: 0.2114\n",
      "Epoch 2/100, Loss: 1.6120, Accuracy: 0.2150\n",
      "Epoch 3/100, Loss: 1.6102, Accuracy: 0.2119\n",
      "Epoch 4/100, Loss: 1.6094, Accuracy: 0.2175\n",
      "Epoch 5/100, Loss: 1.6095, Accuracy: 0.2132\n",
      "Epoch 6/100, Loss: 1.6095, Accuracy: 0.2130\n",
      "Epoch 7/100, Loss: 1.6092, Accuracy: 0.2152\n",
      "Epoch 8/100, Loss: 1.6091, Accuracy: 0.2164\n",
      "Epoch 9/100, Loss: 1.6089, Accuracy: 0.2202\n",
      "Epoch 10/100, Loss: 1.6087, Accuracy: 0.2200\n",
      "Epoch 11/100, Loss: 1.6086, Accuracy: 0.2198\n",
      "Epoch 12/100, Loss: 1.6085, Accuracy: 0.2188\n",
      "Epoch 13/100, Loss: 1.6085, Accuracy: 0.2180\n",
      "Epoch 14/100, Loss: 1.6083, Accuracy: 0.2196\n",
      "Epoch 15/100, Loss: 1.6081, Accuracy: 0.2214\n",
      "Epoch 16/100, Loss: 1.6080, Accuracy: 0.2234\n",
      "Epoch 17/100, Loss: 1.6079, Accuracy: 0.2249\n",
      "Epoch 18/100, Loss: 1.6078, Accuracy: 0.2227\n",
      "Epoch 19/100, Loss: 1.6076, Accuracy: 0.2205\n",
      "Epoch 20/100, Loss: 1.6076, Accuracy: 0.2229\n",
      "Epoch 21/100, Loss: 1.6074, Accuracy: 0.2254\n",
      "Epoch 22/100, Loss: 1.6074, Accuracy: 0.2259\n",
      "Epoch 23/100, Loss: 1.6072, Accuracy: 0.2281\n",
      "Epoch 24/100, Loss: 1.6070, Accuracy: 0.2297\n",
      "Epoch 25/100, Loss: 1.6069, Accuracy: 0.2277\n",
      "Epoch 26/100, Loss: 1.6068, Accuracy: 0.2293\n",
      "Epoch 27/100, Loss: 1.6067, Accuracy: 0.2313\n",
      "Epoch 28/100, Loss: 1.6065, Accuracy: 0.2335\n",
      "Epoch 29/100, Loss: 1.6064, Accuracy: 0.2337\n",
      "Epoch 30/100, Loss: 1.6063, Accuracy: 0.2331\n",
      "Epoch 31/100, Loss: 1.6062, Accuracy: 0.2338\n",
      "Epoch 32/100, Loss: 1.6060, Accuracy: 0.2310\n",
      "Epoch 33/100, Loss: 1.6060, Accuracy: 0.2372\n",
      "Epoch 34/100, Loss: 1.6058, Accuracy: 0.2364\n",
      "Epoch 35/100, Loss: 1.6057, Accuracy: 0.2369\n",
      "Epoch 36/100, Loss: 1.6055, Accuracy: 0.2360\n",
      "Epoch 37/100, Loss: 1.6055, Accuracy: 0.2376\n",
      "Epoch 38/100, Loss: 1.6053, Accuracy: 0.2408\n",
      "Epoch 39/100, Loss: 1.6052, Accuracy: 0.2385\n",
      "Epoch 40/100, Loss: 1.6050, Accuracy: 0.2405\n",
      "Epoch 41/100, Loss: 1.6049, Accuracy: 0.2394\n",
      "Epoch 42/100, Loss: 1.6048, Accuracy: 0.2437\n",
      "Epoch 43/100, Loss: 1.6048, Accuracy: 0.2416\n",
      "Epoch 44/100, Loss: 1.6046, Accuracy: 0.2398\n",
      "Epoch 45/100, Loss: 1.6045, Accuracy: 0.2452\n",
      "Epoch 46/100, Loss: 1.6043, Accuracy: 0.2430\n",
      "Epoch 47/100, Loss: 1.6042, Accuracy: 0.2443\n",
      "Epoch 48/100, Loss: 1.6041, Accuracy: 0.2466\n",
      "Epoch 49/100, Loss: 1.6040, Accuracy: 0.2455\n",
      "Epoch 50/100, Loss: 1.6038, Accuracy: 0.2473\n",
      "Epoch 51/100, Loss: 1.6037, Accuracy: 0.2500\n",
      "Epoch 52/100, Loss: 1.6036, Accuracy: 0.2473\n",
      "Epoch 53/100, Loss: 1.6035, Accuracy: 0.2489\n",
      "Epoch 54/100, Loss: 1.6034, Accuracy: 0.2487\n",
      "Epoch 55/100, Loss: 1.6032, Accuracy: 0.2514\n",
      "Epoch 56/100, Loss: 1.6031, Accuracy: 0.2489\n",
      "Epoch 57/100, Loss: 1.6030, Accuracy: 0.2484\n",
      "Epoch 58/100, Loss: 1.6029, Accuracy: 0.2520\n",
      "Epoch 59/100, Loss: 1.6027, Accuracy: 0.2531\n",
      "Epoch 60/100, Loss: 1.6026, Accuracy: 0.2552\n",
      "Epoch 61/100, Loss: 1.6025, Accuracy: 0.2574\n",
      "Epoch 62/100, Loss: 1.6023, Accuracy: 0.2592\n",
      "Epoch 63/100, Loss: 1.6022, Accuracy: 0.2565\n",
      "Epoch 64/100, Loss: 1.6021, Accuracy: 0.2534\n",
      "Epoch 65/100, Loss: 1.6020, Accuracy: 0.2565\n",
      "Epoch 66/100, Loss: 1.6018, Accuracy: 0.2613\n",
      "Epoch 67/100, Loss: 1.6017, Accuracy: 0.2599\n",
      "Epoch 68/100, Loss: 1.6016, Accuracy: 0.2629\n",
      "Epoch 69/100, Loss: 1.6015, Accuracy: 0.2651\n",
      "Epoch 70/100, Loss: 1.6013, Accuracy: 0.2636\n",
      "Epoch 71/100, Loss: 1.6012, Accuracy: 0.2653\n",
      "Epoch 72/100, Loss: 1.6012, Accuracy: 0.2644\n",
      "Epoch 73/100, Loss: 1.6010, Accuracy: 0.2633\n",
      "Epoch 74/100, Loss: 1.6009, Accuracy: 0.2626\n",
      "Epoch 75/100, Loss: 1.6007, Accuracy: 0.2631\n",
      "Epoch 76/100, Loss: 1.6006, Accuracy: 0.2649\n",
      "Epoch 77/100, Loss: 1.6005, Accuracy: 0.2681\n",
      "Epoch 78/100, Loss: 1.6003, Accuracy: 0.2699\n",
      "Epoch 79/100, Loss: 1.6002, Accuracy: 0.2649\n",
      "Epoch 80/100, Loss: 1.6001, Accuracy: 0.2690\n",
      "Epoch 81/100, Loss: 1.6000, Accuracy: 0.2717\n",
      "Epoch 82/100, Loss: 1.5999, Accuracy: 0.2733\n",
      "Epoch 83/100, Loss: 1.5997, Accuracy: 0.2741\n",
      "Epoch 84/100, Loss: 1.5996, Accuracy: 0.2746\n",
      "Epoch 85/100, Loss: 1.5995, Accuracy: 0.2744\n",
      "Epoch 86/100, Loss: 1.5994, Accuracy: 0.2744\n",
      "Epoch 87/100, Loss: 1.5993, Accuracy: 0.2757\n",
      "Epoch 88/100, Loss: 1.5991, Accuracy: 0.2769\n",
      "Epoch 89/100, Loss: 1.5991, Accuracy: 0.2689\n",
      "Epoch 90/100, Loss: 1.5989, Accuracy: 0.2753\n",
      "Epoch 91/100, Loss: 1.5987, Accuracy: 0.2771\n",
      "Epoch 92/100, Loss: 1.5986, Accuracy: 0.2759\n",
      "Epoch 93/100, Loss: 1.5986, Accuracy: 0.2814\n",
      "Epoch 94/100, Loss: 1.5985, Accuracy: 0.2823\n",
      "Epoch 95/100, Loss: 1.5983, Accuracy: 0.2791\n",
      "Epoch 96/100, Loss: 1.5982, Accuracy: 0.2742\n",
      "Epoch 97/100, Loss: 1.5980, Accuracy: 0.2721\n",
      "Epoch 98/100, Loss: 1.5980, Accuracy: 0.2791\n",
      "Epoch 99/100, Loss: 1.5977, Accuracy: 0.2811\n",
      "Epoch 100/100, Loss: 1.5976, Accuracy: 0.2827\n",
      "Validation Accuracy: 0.2011\n",
      "Fold 4/5\n",
      "Epoch 1/100, Loss: 1.6177, Accuracy: 0.1956\n",
      "Epoch 2/100, Loss: 1.6128, Accuracy: 0.1965\n",
      "Epoch 3/100, Loss: 1.6103, Accuracy: 0.2008\n",
      "Epoch 4/100, Loss: 1.6101, Accuracy: 0.1999\n",
      "Epoch 5/100, Loss: 1.6103, Accuracy: 0.2024\n",
      "Epoch 6/100, Loss: 1.6099, Accuracy: 0.2037\n",
      "Epoch 7/100, Loss: 1.6095, Accuracy: 0.2062\n",
      "Epoch 8/100, Loss: 1.6093, Accuracy: 0.2080\n",
      "Epoch 9/100, Loss: 1.6092, Accuracy: 0.2071\n",
      "Epoch 10/100, Loss: 1.6091, Accuracy: 0.2074\n",
      "Epoch 11/100, Loss: 1.6089, Accuracy: 0.2051\n",
      "Epoch 12/100, Loss: 1.6087, Accuracy: 0.2083\n",
      "Epoch 13/100, Loss: 1.6087, Accuracy: 0.2042\n",
      "Epoch 14/100, Loss: 1.6086, Accuracy: 0.2060\n",
      "Epoch 15/100, Loss: 1.6084, Accuracy: 0.2116\n",
      "Epoch 16/100, Loss: 1.6083, Accuracy: 0.2128\n",
      "Epoch 17/100, Loss: 1.6081, Accuracy: 0.2152\n",
      "Epoch 18/100, Loss: 1.6080, Accuracy: 0.2162\n",
      "Epoch 19/100, Loss: 1.6078, Accuracy: 0.2191\n",
      "Epoch 20/100, Loss: 1.6078, Accuracy: 0.2186\n",
      "Epoch 21/100, Loss: 1.6075, Accuracy: 0.2114\n",
      "Epoch 22/100, Loss: 1.6075, Accuracy: 0.2177\n",
      "Epoch 23/100, Loss: 1.6073, Accuracy: 0.2173\n",
      "Epoch 24/100, Loss: 1.6072, Accuracy: 0.2193\n",
      "Epoch 25/100, Loss: 1.6071, Accuracy: 0.2211\n",
      "Epoch 26/100, Loss: 1.6069, Accuracy: 0.2236\n",
      "Epoch 27/100, Loss: 1.6068, Accuracy: 0.2207\n",
      "Epoch 28/100, Loss: 1.6067, Accuracy: 0.2218\n",
      "Epoch 29/100, Loss: 1.6066, Accuracy: 0.2236\n",
      "Epoch 30/100, Loss: 1.6064, Accuracy: 0.2223\n",
      "Epoch 31/100, Loss: 1.6063, Accuracy: 0.2261\n",
      "Epoch 32/100, Loss: 1.6061, Accuracy: 0.2245\n",
      "Epoch 33/100, Loss: 1.6060, Accuracy: 0.2249\n",
      "Epoch 34/100, Loss: 1.6059, Accuracy: 0.2268\n",
      "Epoch 35/100, Loss: 1.6057, Accuracy: 0.2265\n",
      "Epoch 36/100, Loss: 1.6056, Accuracy: 0.2225\n",
      "Epoch 37/100, Loss: 1.6055, Accuracy: 0.2254\n",
      "Epoch 38/100, Loss: 1.6054, Accuracy: 0.2272\n",
      "Epoch 39/100, Loss: 1.6052, Accuracy: 0.2267\n",
      "Epoch 40/100, Loss: 1.6050, Accuracy: 0.2340\n",
      "Epoch 41/100, Loss: 1.6050, Accuracy: 0.2360\n",
      "Epoch 42/100, Loss: 1.6049, Accuracy: 0.2405\n",
      "Epoch 43/100, Loss: 1.6047, Accuracy: 0.2387\n",
      "Epoch 44/100, Loss: 1.6045, Accuracy: 0.2353\n",
      "Epoch 45/100, Loss: 1.6044, Accuracy: 0.2358\n",
      "Epoch 46/100, Loss: 1.6043, Accuracy: 0.2380\n",
      "Epoch 47/100, Loss: 1.6042, Accuracy: 0.2383\n",
      "Epoch 48/100, Loss: 1.6040, Accuracy: 0.2410\n",
      "Epoch 49/100, Loss: 1.6039, Accuracy: 0.2408\n",
      "Epoch 50/100, Loss: 1.6037, Accuracy: 0.2446\n",
      "Epoch 51/100, Loss: 1.6037, Accuracy: 0.2435\n",
      "Epoch 52/100, Loss: 1.6035, Accuracy: 0.2428\n",
      "Epoch 53/100, Loss: 1.6035, Accuracy: 0.2437\n",
      "Epoch 54/100, Loss: 1.6033, Accuracy: 0.2432\n",
      "Epoch 55/100, Loss: 1.6031, Accuracy: 0.2446\n",
      "Epoch 56/100, Loss: 1.6029, Accuracy: 0.2459\n",
      "Epoch 57/100, Loss: 1.6028, Accuracy: 0.2468\n",
      "Epoch 58/100, Loss: 1.6026, Accuracy: 0.2507\n",
      "Epoch 59/100, Loss: 1.6026, Accuracy: 0.2443\n",
      "Epoch 60/100, Loss: 1.6025, Accuracy: 0.2452\n",
      "Epoch 61/100, Loss: 1.6023, Accuracy: 0.2513\n",
      "Epoch 62/100, Loss: 1.6022, Accuracy: 0.2529\n",
      "Epoch 63/100, Loss: 1.6022, Accuracy: 0.2518\n",
      "Epoch 64/100, Loss: 1.6019, Accuracy: 0.2480\n",
      "Epoch 65/100, Loss: 1.6018, Accuracy: 0.2520\n",
      "Epoch 66/100, Loss: 1.6016, Accuracy: 0.2523\n",
      "Epoch 67/100, Loss: 1.6015, Accuracy: 0.2493\n",
      "Epoch 68/100, Loss: 1.6015, Accuracy: 0.2520\n",
      "Epoch 69/100, Loss: 1.6012, Accuracy: 0.2538\n",
      "Epoch 70/100, Loss: 1.6011, Accuracy: 0.2534\n",
      "Epoch 71/100, Loss: 1.6009, Accuracy: 0.2550\n",
      "Epoch 72/100, Loss: 1.6009, Accuracy: 0.2599\n",
      "Epoch 73/100, Loss: 1.6007, Accuracy: 0.2622\n",
      "Epoch 74/100, Loss: 1.6006, Accuracy: 0.2588\n",
      "Epoch 75/100, Loss: 1.6004, Accuracy: 0.2590\n",
      "Epoch 76/100, Loss: 1.6004, Accuracy: 0.2581\n",
      "Epoch 77/100, Loss: 1.6002, Accuracy: 0.2599\n",
      "Epoch 78/100, Loss: 1.6001, Accuracy: 0.2636\n",
      "Epoch 79/100, Loss: 1.5999, Accuracy: 0.2663\n",
      "Epoch 80/100, Loss: 1.5998, Accuracy: 0.2647\n",
      "Epoch 81/100, Loss: 1.5996, Accuracy: 0.2611\n",
      "Epoch 82/100, Loss: 1.5995, Accuracy: 0.2653\n",
      "Epoch 83/100, Loss: 1.5994, Accuracy: 0.2658\n",
      "Epoch 84/100, Loss: 1.5992, Accuracy: 0.2687\n",
      "Epoch 85/100, Loss: 1.5991, Accuracy: 0.2672\n",
      "Epoch 86/100, Loss: 1.5990, Accuracy: 0.2685\n",
      "Epoch 87/100, Loss: 1.5989, Accuracy: 0.2671\n",
      "Epoch 88/100, Loss: 1.5987, Accuracy: 0.2703\n",
      "Epoch 89/100, Loss: 1.5986, Accuracy: 0.2703\n",
      "Epoch 90/100, Loss: 1.5984, Accuracy: 0.2701\n",
      "Epoch 91/100, Loss: 1.5983, Accuracy: 0.2750\n",
      "Epoch 92/100, Loss: 1.5982, Accuracy: 0.2723\n",
      "Epoch 93/100, Loss: 1.5981, Accuracy: 0.2726\n",
      "Epoch 94/100, Loss: 1.5979, Accuracy: 0.2721\n",
      "Epoch 95/100, Loss: 1.5978, Accuracy: 0.2769\n",
      "Epoch 96/100, Loss: 1.5977, Accuracy: 0.2762\n",
      "Epoch 97/100, Loss: 1.5975, Accuracy: 0.2780\n",
      "Epoch 98/100, Loss: 1.5974, Accuracy: 0.2784\n",
      "Epoch 99/100, Loss: 1.5973, Accuracy: 0.2793\n",
      "Epoch 100/100, Loss: 1.5971, Accuracy: 0.2802\n",
      "Validation Accuracy: 0.1997\n",
      "Fold 5/5\n",
      "Epoch 1/100, Loss: 1.6128, Accuracy: 0.2080\n",
      "Epoch 2/100, Loss: 1.6117, Accuracy: 0.2087\n",
      "Epoch 3/100, Loss: 1.6105, Accuracy: 0.2091\n",
      "Epoch 4/100, Loss: 1.6099, Accuracy: 0.2087\n",
      "Epoch 5/100, Loss: 1.6096, Accuracy: 0.2047\n",
      "Epoch 6/100, Loss: 1.6096, Accuracy: 0.2058\n",
      "Epoch 7/100, Loss: 1.6095, Accuracy: 0.2087\n",
      "Epoch 8/100, Loss: 1.6094, Accuracy: 0.2099\n",
      "Epoch 9/100, Loss: 1.6093, Accuracy: 0.2121\n",
      "Epoch 10/100, Loss: 1.6092, Accuracy: 0.2126\n",
      "Epoch 11/100, Loss: 1.6090, Accuracy: 0.2117\n",
      "Epoch 12/100, Loss: 1.6089, Accuracy: 0.2135\n",
      "Epoch 13/100, Loss: 1.6088, Accuracy: 0.2130\n",
      "Epoch 14/100, Loss: 1.6087, Accuracy: 0.2098\n",
      "Epoch 15/100, Loss: 1.6086, Accuracy: 0.2080\n",
      "Epoch 16/100, Loss: 1.6085, Accuracy: 0.2114\n",
      "Epoch 17/100, Loss: 1.6084, Accuracy: 0.2105\n",
      "Epoch 18/100, Loss: 1.6083, Accuracy: 0.2110\n",
      "Epoch 19/100, Loss: 1.6082, Accuracy: 0.2141\n",
      "Epoch 20/100, Loss: 1.6080, Accuracy: 0.2164\n",
      "Epoch 21/100, Loss: 1.6079, Accuracy: 0.2159\n",
      "Epoch 22/100, Loss: 1.6078, Accuracy: 0.2139\n",
      "Epoch 23/100, Loss: 1.6077, Accuracy: 0.2148\n",
      "Epoch 24/100, Loss: 1.6076, Accuracy: 0.2175\n",
      "Epoch 25/100, Loss: 1.6075, Accuracy: 0.2189\n",
      "Epoch 26/100, Loss: 1.6074, Accuracy: 0.2198\n",
      "Epoch 27/100, Loss: 1.6073, Accuracy: 0.2179\n",
      "Epoch 28/100, Loss: 1.6072, Accuracy: 0.2177\n",
      "Epoch 29/100, Loss: 1.6071, Accuracy: 0.2196\n",
      "Epoch 30/100, Loss: 1.6070, Accuracy: 0.2207\n",
      "Epoch 31/100, Loss: 1.6069, Accuracy: 0.2198\n",
      "Epoch 32/100, Loss: 1.6067, Accuracy: 0.2222\n",
      "Epoch 33/100, Loss: 1.6067, Accuracy: 0.2214\n",
      "Epoch 34/100, Loss: 1.6065, Accuracy: 0.2229\n",
      "Epoch 35/100, Loss: 1.6064, Accuracy: 0.2229\n",
      "Epoch 36/100, Loss: 1.6063, Accuracy: 0.2263\n",
      "Epoch 37/100, Loss: 1.6062, Accuracy: 0.2276\n",
      "Epoch 38/100, Loss: 1.6061, Accuracy: 0.2249\n",
      "Epoch 39/100, Loss: 1.6060, Accuracy: 0.2245\n",
      "Epoch 40/100, Loss: 1.6059, Accuracy: 0.2286\n",
      "Epoch 41/100, Loss: 1.6058, Accuracy: 0.2256\n",
      "Epoch 42/100, Loss: 1.6057, Accuracy: 0.2249\n",
      "Epoch 43/100, Loss: 1.6056, Accuracy: 0.2277\n",
      "Epoch 44/100, Loss: 1.6054, Accuracy: 0.2293\n",
      "Epoch 45/100, Loss: 1.6053, Accuracy: 0.2272\n",
      "Epoch 46/100, Loss: 1.6052, Accuracy: 0.2306\n",
      "Epoch 47/100, Loss: 1.6051, Accuracy: 0.2324\n",
      "Epoch 48/100, Loss: 1.6050, Accuracy: 0.2324\n",
      "Epoch 49/100, Loss: 1.6049, Accuracy: 0.2353\n",
      "Epoch 50/100, Loss: 1.6048, Accuracy: 0.2353\n",
      "Epoch 51/100, Loss: 1.6047, Accuracy: 0.2322\n",
      "Epoch 52/100, Loss: 1.6046, Accuracy: 0.2333\n",
      "Epoch 53/100, Loss: 1.6045, Accuracy: 0.2335\n",
      "Epoch 54/100, Loss: 1.6044, Accuracy: 0.2311\n",
      "Epoch 55/100, Loss: 1.6042, Accuracy: 0.2335\n",
      "Epoch 56/100, Loss: 1.6041, Accuracy: 0.2365\n",
      "Epoch 57/100, Loss: 1.6040, Accuracy: 0.2381\n",
      "Epoch 58/100, Loss: 1.6039, Accuracy: 0.2362\n",
      "Epoch 59/100, Loss: 1.6038, Accuracy: 0.2383\n",
      "Epoch 60/100, Loss: 1.6037, Accuracy: 0.2432\n",
      "Epoch 61/100, Loss: 1.6036, Accuracy: 0.2421\n",
      "Epoch 62/100, Loss: 1.6035, Accuracy: 0.2428\n",
      "Epoch 63/100, Loss: 1.6034, Accuracy: 0.2401\n",
      "Epoch 64/100, Loss: 1.6033, Accuracy: 0.2405\n",
      "Epoch 65/100, Loss: 1.6032, Accuracy: 0.2392\n",
      "Epoch 66/100, Loss: 1.6030, Accuracy: 0.2376\n",
      "Epoch 67/100, Loss: 1.6030, Accuracy: 0.2355\n",
      "Epoch 68/100, Loss: 1.6029, Accuracy: 0.2346\n",
      "Epoch 69/100, Loss: 1.6027, Accuracy: 0.2378\n",
      "Epoch 70/100, Loss: 1.6026, Accuracy: 0.2428\n",
      "Epoch 71/100, Loss: 1.6025, Accuracy: 0.2464\n",
      "Epoch 72/100, Loss: 1.6024, Accuracy: 0.2457\n",
      "Epoch 73/100, Loss: 1.6023, Accuracy: 0.2477\n",
      "Epoch 74/100, Loss: 1.6022, Accuracy: 0.2478\n",
      "Epoch 75/100, Loss: 1.6021, Accuracy: 0.2496\n",
      "Epoch 76/100, Loss: 1.6020, Accuracy: 0.2505\n",
      "Epoch 77/100, Loss: 1.6018, Accuracy: 0.2486\n",
      "Epoch 78/100, Loss: 1.6017, Accuracy: 0.2477\n",
      "Epoch 79/100, Loss: 1.6016, Accuracy: 0.2478\n",
      "Epoch 80/100, Loss: 1.6015, Accuracy: 0.2457\n",
      "Epoch 81/100, Loss: 1.6014, Accuracy: 0.2450\n",
      "Epoch 82/100, Loss: 1.6013, Accuracy: 0.2495\n",
      "Epoch 83/100, Loss: 1.6012, Accuracy: 0.2514\n",
      "Epoch 84/100, Loss: 1.6011, Accuracy: 0.2529\n",
      "Epoch 85/100, Loss: 1.6010, Accuracy: 0.2545\n",
      "Epoch 86/100, Loss: 1.6009, Accuracy: 0.2543\n",
      "Epoch 87/100, Loss: 1.6008, Accuracy: 0.2550\n",
      "Epoch 88/100, Loss: 1.6006, Accuracy: 0.2556\n",
      "Epoch 89/100, Loss: 1.6005, Accuracy: 0.2577\n",
      "Epoch 90/100, Loss: 1.6005, Accuracy: 0.2534\n",
      "Epoch 91/100, Loss: 1.6004, Accuracy: 0.2566\n",
      "Epoch 92/100, Loss: 1.6002, Accuracy: 0.2570\n",
      "Epoch 93/100, Loss: 1.6001, Accuracy: 0.2593\n",
      "Epoch 94/100, Loss: 1.6000, Accuracy: 0.2604\n",
      "Epoch 95/100, Loss: 1.5999, Accuracy: 0.2602\n",
      "Epoch 96/100, Loss: 1.5998, Accuracy: 0.2611\n",
      "Epoch 97/100, Loss: 1.5997, Accuracy: 0.2622\n",
      "Epoch 98/100, Loss: 1.5996, Accuracy: 0.2606\n",
      "Epoch 99/100, Loss: 1.5994, Accuracy: 0.2626\n",
      "Epoch 100/100, Loss: 1.5994, Accuracy: 0.2584\n",
      "Validation Accuracy: 0.1925\n",
      "Test Accuracy: 0.1902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the Conv1D model\n",
    "class Conv1D_model(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Conv1D_model, self).__init__()\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(22336, 512)  \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to 1D\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Define learning parameters\n",
    "lr = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 1024\n",
    "num_classes = 5\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define training and testing tensors (replace with your actual data)\n",
    "X_train_tensor = torch.randn(6960, 1, 2816)\n",
    "y_train_tensor = torch.randint(0, num_classes, (6960,))\n",
    "X_test_tensor = torch.randn(1740, 1, 2816)\n",
    "y_test_tensor = torch.randint(0, num_classes, (1740,))\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_tensor)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = X_train_tensor[train_index], X_train_tensor[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_tensor[train_index], y_train_tensor[val_index]\n",
    "\n",
    "    # Create DataLoader for training and validation sets\n",
    "    train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Conv1D_model(num_classes=num_classes)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1902\n",
      "MAE: 1.4368\n",
      "RMSE: 1.7590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ens/AU59350/anaconda3/envs/ClaraEnv/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "    y_test_tensor_sampled = y_test_tensor[:len(predictions)]\n",
    "    accuracy = torch.mean((predictions == y_test_tensor_sampled).float()).item()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Calculate MAE and RMSE\n",
    "    mae = mean_absolute_error(y_test_tensor_sampled, predictions)\n",
    "    rmse = mean_squared_error(y_test_tensor_sampled, predictions, squared=False)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining model classes and utility functions\n",
    "============================================\n",
    "\n",
    "Next, I need to define our model classes. Several user-defined\n",
    "parameters need to be set here. I use two different architectures,\n",
    "keeping the number of filters fixed across our experiments to ensure\n",
    "fair comparisons. Both architectures are Convolutional Neural Networks\n",
    "(CNNs) with a different number of convolutional layers that serve as\n",
    "feature extractors, followed by a classifier with 10 classes. The number\n",
    "of filters and neurons is smaller for the students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deeper neural network class to be used as teacher:\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Lightweight neural network class to be used as student:\n",
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I employ 2 functions to help produce and evaluate the results on the\n",
    "original classification task. One function is called `train` and takes\n",
    "the following arguments:\n",
    "\n",
    "-   `model`: A model instance to train (update its weights) via this\n",
    "    function.\n",
    "-   `train_loader`: We defined our `train_loader` above, and its job is\n",
    "    to feed the data into the model.\n",
    "-   `epochs`: How many times we loop over the dataset.\n",
    "-   `learning_rate`: The learning rate determines how large our steps\n",
    "    towards convergence should be. Too large or too small steps can be\n",
    "    detrimental.\n",
    "-   `device`: Determines the device to run the workload on. Can be\n",
    "    either CPU or GPU depending on availability.\n",
    "\n",
    "Our test function is similar, but it will be invoked with `test_loader`\n",
    "to load images from the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy runs\n",
    "==================\n",
    "\n",
    "For reproducibility, we need to set the torch manual seed. I train\n",
    "networks using different methods, so to compare them fairly, it makes\n",
    "sense to initialize the networks with the same weights. I start by\n",
    "training the teacher network using cross-entropy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3609035784936012\n",
      "Epoch 2/10, Loss: 0.8999937804763579\n",
      "Epoch 3/10, Loss: 0.698583966935687\n",
      "Epoch 4/10, Loss: 0.5613005085064627\n",
      "Epoch 5/10, Loss: 0.4368233955119882\n",
      "Epoch 6/10, Loss: 0.3273872912997175\n",
      "Epoch 7/10, Loss: 0.23821778524943324\n",
      "Epoch 8/10, Loss: 0.18469191213016925\n",
      "Epoch 9/10, Loss: 0.14930837982527131\n",
      "Epoch 10/10, Loss: 0.12943355578100285\n",
      "Test Accuracy: 75.09%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "# Instantiate the lightweight network:\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I instantiate one more lightweight network model to compare their\n",
    "performances. Back propagation is sensitive to weight initialization, so\n",
    "I need to make sure these two networks have the exact same\n",
    "initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure I have created a copy of the first network, we inspect the\n",
    "norm of its first layer. If it matches, then the networks are indeed the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of parameters in each model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test the lightweight network with cross entropy loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, based on test accuracy, I can now compare the deeper\n",
    "network that is to be used as a teacher with the lightweight network\n",
    "that is the supposed student. So far, the student has not intervened\n",
    "with the teacher, therefore this performance is achieved by the student\n",
    "itself. The metrics so far can be seen with the following lines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - soft_prob)) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine loss minimization run\n",
    "============================\n",
    "\n",
    "The temperature parameter controls the softness of the softmax function and the loss coefficients. In\n",
    "neural networks, it is easy to include to include additional loss\n",
    "functions to the main objectives to achieve goals like better\n",
    "generalization.   Let\\'s try including an objective for the student, but\n",
    "now let\\'s focus on their hidden states rather than their output layers.  \n",
    "The goal is to convey information from the teacher\\'s representation to\n",
    "the student by including a naive loss function, whose minimization\n",
    "implies that the flattened vectors that are subsequently passed to the\n",
    "classifiers have become more *similar* as the loss decreases. Of course,\n",
    "the teacher does not update its weights, so the minimization depends\n",
    "only on the student\\'s weights.   The rationale behind this method is that\n",
    "I am  operating under the assumption that the teacher model has a\n",
    "better internal representation that is unlikely to be achieved by the\n",
    "student without external intervention, therefore we artificially push\n",
    "the student to mimic the internal representation of the teacher. Whether\n",
    "or not this will end up helping the student is not straightforward,\n",
    "though, because pushing the lightweight network to reach this point\n",
    "could be a good thing, assuming that I have found an internal\n",
    "representation that leads to better test accuracy, but it could also be\n",
    "harmful because the networks have different architectures and the\n",
    "student does not have the same learning capacity as the teacher. In\n",
    "other words, there is no reason for these two vectors, the student\\'s\n",
    "and the teacher\\'s to match per component. The student could reach an\n",
    "internal representation that is a permutation of the teacher\\'s and it\n",
    "would be just as efficient. Nonetheless, we can still run a quick\n",
    "experiment to figure out the impact of this method. We will be using the\n",
    "`CosineEmbeddingLoss` which is given by the following formula:\n",
    "\n",
    "![Formula for\n",
    "CosineEmbeddingLoss](https://pytorch.org/tutorials//../_static/img/knowledge_distillation/cosine_embedding_loss.png){.align-center\n",
    "width=\"450px\"}\n",
    "\n",
    "Obviously, there is one thing that I need to resolve first. When we\n",
    "applied distillation to the output layer we mentioned that both networks\n",
    "have the same number of neurons, equal to the number of classes.\n",
    "However, this is not the case for the layer following our convolutional\n",
    "layers. Here, the teacher has more neurons than the student after the\n",
    "flattening of the final convolutional layer. Our loss function accepts\n",
    "two vectors of equal dimensionality as inputs, therefore we need to\n",
    "somehow match them. We will solve this by including an average pooling\n",
    "layer after the teacher\\'s convolutional layer to reduce its\n",
    "dimensionality to match that of the student.\n",
    "\n",
    "To proceed, I will modify our model classes, or create new ones. Now,\n",
    "the forward function returns not only the logits of the network but also\n",
    "the flattened hidden representation after the convolutional layer. We\n",
    "include the aforementioned pooling for the modified teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModifiedDeepNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
    "        return x, flattened_conv_output_after_pooling\n",
    "\n",
    "# Create a similar student class where we return a tuple. We do not apply pooling after flattening.\n",
    "class ModifiedLightNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        return x, flattened_conv_output\n",
    "\n",
    "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
    "modified_nn_deep = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
    "modified_nn_deep.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "# Once again ensure the norm of the first layer is the same for both networks\n",
    "print(\"Norm of 1st layer for deep_nn:\", torch.norm(nn_deep.features[0].weight).item())\n",
    "print(\"Norm of 1st layer for modified_deep_nn:\", torch.norm(modified_nn_deep.features[0].weight).item())\n",
    "\n",
    "# Initialize a modified lightweight network with the same seed as our other lightweight instances. This will be trained from scratch to examine the effectiveness of cosine loss minimization.\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light = ModifiedLightNNCosine(num_classes=10).to(device)\n",
    "print(\"Norm of 1st layer:\", torch.norm(modified_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, I need to change the train loop because now the model\n",
    "returns a tuple `(logits, hidden_representation)`. Using a sample input\n",
    "tensor we can print their shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a sample input tensor\n",
    "sample_input = torch.randn(128, 3, 32, 32).to(device) # Batch size: 128, Filters: 3, Image size: 32x32\n",
    "\n",
    "# Pass the input through the student\n",
    "logits, hidden_representation = modified_nn_light(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
    "\n",
    "# Pass the input through the teacher\n",
    "logits, hidden_representation = modified_nn_deep(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model and keep only the hidden representation\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "\n",
    "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is the case where loss minimization leads to cosine similarity increase.\n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to modify our test function for the same reason. Here I ignore\n",
    "the hidden representation returned by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_multiple_outputs(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I include both knowledge distillation and\n",
    "cosine loss minimization in the same function. It is common to combine\n",
    "methods to achieve better performance in teacher-student paradigms. For\n",
    "now, we can run a simple train-test session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test the lightweight network with cross entropy loss\n",
    "train_cosine_loss(teacher=modified_nn_deep, student=modified_nn_light, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_cosine_loss = test_multiple_outputs(modified_nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate regressor run\n",
    "==========================\n",
    "\n",
    "The naive minimization does not guarantee better results for several\n",
    "reasons, one being the dimensionality of the vectors. Cosine similarity\n",
    "generally works better than Euclidean distance for vectors of higher\n",
    "dimensionality, but I am dealing with vectors with 1024 components\n",
    "each, so it is much harder to extract meaningful similarities.\n",
    "Furthermore, as we mentioned, pushing towards a match of the hidden\n",
    "representation of the teacher and the student is not supported by\n",
    "theory. There are no good reasons why we should be aiming for a 1:1\n",
    "match of these vectors. I will provide a final example of training\n",
    "intervention by including an extra network called regressor. The\n",
    "objective is to first extract the feature map of the teacher after a\n",
    "convolutional layer, then extract a feature map of the student after a\n",
    "convolutional layer, and finally try to match these maps. However, this\n",
    "time, we will introduce a regressor between the networks to facilitate\n",
    "the matching process. The regressor will be trainable and ideally will\n",
    "do a better job than our naive cosine loss minimization scheme. Its main\n",
    "job is to match the dimensionality of these feature maps so that we can\n",
    "properly define a loss function between the teacher and the student.\n",
    "Defining such a loss function provides a teaching \\\"path,\\\" which is\n",
    "basically a flow to back-propagate gradients that will change the\n",
    "student\\'s weights. Focusing on the output of the convolutional layers\n",
    "right before each classifier for our original networks, I have the\n",
    "following shapes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pass the sample input only from the convolutional feature extractor\n",
    "convolutional_fe_output_student = nn_light.features(sample_input)\n",
    "convolutional_fe_output_teacher = nn_deep.features(sample_input)\n",
    "\n",
    "# Print their shapes\n",
    "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
    "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModifiedDeepNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        conv_feature_map = x\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_feature_map\n",
    "\n",
    "class ModifiedLightNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Include an extra regressor (in our case linear)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        regressor_output = self.regressor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, regressor_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I have to update the train loop again. This time, I\n",
    "extract the regressor output of the student, the feature map of the\n",
    "teacher, I calculate the `MSE` on these tensors (they have the exact\n",
    "same shape so it\\'s properly defined) and we back propagate gradients\n",
    "based on that loss, in addition to the regular cross entropy loss of the\n",
    "classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_mse_loss(teacher, student, train_loader, epochs, learning_rate, feature_map_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Again ignore teacher logits\n",
    "            with torch.no_grad():\n",
    "                _, teacher_feature_map = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, regressor_feature_map = student(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Notice how our test function remains the same here with the one we used in our previous case. We only care about the actual outputs because we measure accuracy.\n",
    "\n",
    "# Initialize a ModifiedLightNNRegressor\n",
    "torch.manual_seed(42)\n",
    "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=10).to(device)\n",
    "\n",
    "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
    "modified_nn_deep_reg = ModifiedDeepNNRegressor(num_classes=10).to(device)\n",
    "modified_nn_deep_reg.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "# Train and test once again\n",
    "train_mse_loss(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, train_loader=train_loader, epochs=10, learning_rate=0.001, feature_map_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_mse_loss = test_multiple_outputs(modified_nn_light_reg, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that the final method will work better than `CosineLoss`\n",
    "because now I have allowed a trainable layer between the teacher and\n",
    "the student, which gives the student some wiggle room when it comes to\n",
    "learning, rather than pushing the student to copy the teacher\\'s\n",
    "representation. Including the extra network is the idea behind\n",
    "hint-based distillation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_light_ce_and_cosine_loss:.2f}%\")\n",
    "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_light_ce_and_mse_loss:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "None of the methods above increases the number of parameters for the\n",
    "network or inference time, so the performance increase comes at the\n",
    "little cost of calculating gradients during training. In ML\n",
    "applications, we mostly care about inference time because training\n",
    "happens before the model deployment. If our lightweight model is still\n",
    "too heavy for deployment, we can apply different ideas, such as\n",
    "post-training quantization. Additional losses can be applied in many\n",
    "tasks, not just classification, and you can experiment with quantities\n",
    "like coefficients, temperature, or number of neurons. Feel free to tune\n",
    "any numbers in the tutorial above, but keep in mind, if you change the\n",
    "number of neurons / filters chances are a shape mismatch might occur.\n",
    "\n",
    "For more information, refers to:\n",
    "\n",
    "-   [Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a\n",
    "    neural network. In: Neural Information Processing System Deep\n",
    "    Learning Workshop (2015)](https://arxiv.org/abs/1503.02531)\n",
    "-   [Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C.,\n",
    "    Bengio, Y.: Fitnets: Hints for thin deep nets. In: Proceedings of\n",
    "    the International Conference on Learning\n",
    "    Representations (2015)](https://arxiv.org/abs/1412.6550)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
